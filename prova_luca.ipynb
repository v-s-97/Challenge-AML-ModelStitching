{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mamiglia/challenge/blob/master/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SYldJ58Zc2gX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYldJ58Zc2gX",
        "outputId": "ac87a904-31f1-4aa9-e8b4-fe1159c48c26"
      },
      "outputs": [],
      "source": [
        "# !mkdir data\n",
        "# !gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
        "# !gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
        "# !unzip -o test.zip -d data\n",
        "# !unzip -o train.zip -d data\n",
        "\n",
        "# !git clone https://github.com/Mamiglia/challenge.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9a8587",
      "metadata": {
        "id": "9c9a8587"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import torch.nn.functional as F \n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from src.common import load_data, prepare_train_data, generate_submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5e8e20",
      "metadata": {
        "id": "9f5e8e20"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_PATH = \"models/mlp_baseline.pth\"\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "LR = 1e-4\n",
        "\n",
        "DEVICE = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f94054ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_data = load_data(\"data/train/train.npz\")\n",
        "# X, y, label = prepare_train_data(train_data)\n",
        "# K = 64  # 32â€“128 funziona bene\n",
        "# kmeans = KMeans(n_clusters=K, n_init=10, random_state=0).fit(X.cpu().numpy())\n",
        "# centroids = torch.tensor(kmeans.cluster_centers_, dtype=X.dtype, device=DEVICE)  # [K, d_in]\n",
        "\n",
        "class LocalAffineMoE(nn.Module):\n",
        "    def __init__(self, d_in=1024, d_out=1536, K=64, temp=0.07, centroids=None):\n",
        "        super().__init__()\n",
        "        self.K = K\n",
        "        self.temp = temp\n",
        "        # per-expert affine\n",
        "        self.W = nn.Parameter(torch.randn(K, d_in, d_out) * (1.0 / math.sqrt(d_in)))\n",
        "        self.b = nn.Parameter(torch.zeros(K, d_out))\n",
        "        # piccolo MLP per gating additivo su distanza dai centroidi\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(d_in, 256), nn.GELU(), nn.Linear(256, K)\n",
        "        )\n",
        "        self.register_buffer(\"centroids\", centroids)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # gating: distanza dai centroidi + logit del piccolo MLP\n",
        "        # (evita che la sola distanza domini)\n",
        "        d2 = torch.cdist(x, self.centroids, p=2).pow(2)                 # [B,K]\n",
        "        g_logits = -d2 / self.temp + self.gate(x)                       # [B,K]\n",
        "        g = torch.softmax(g_logits, dim=-1)                             # [B,K]\n",
        "\n",
        "        # per-expert affine e somma pesata\n",
        "        # out_k = x @ W_k + b_k  -> [B,K,d_out]\n",
        "        out_k = torch.einsum('bd,kdo->bko', x, self.W) + self.b         # [B,K,D]\n",
        "        out = torch.einsum('bk,bkd->bd', g, out_k)                      # [B,D]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8952b1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def csls_similarity(z_pred, z_true, k=10):\n",
        "    zp = F.normalize(z_pred, dim=-1); zt = F.normalize(z_true, dim=-1)\n",
        "    sim = zp @ zt.T\n",
        "    with torch.no_grad():\n",
        "        r_q = sim.topk(k, dim=1).values.mean(1, keepdim=True)\n",
        "        r_g = sim.topk(k, dim=0).values.mean(0, keepdim=True)\n",
        "    return 2*sim - r_q - r_g\n",
        "\n",
        "def hybrid_loss(z_pred, z_true, temperature=0.15, alpha=0.2, beta=0.6, gamma=1.5, hard_neg_weight=0.1):\n",
        "    mse = F.mse_loss(z_pred, z_true)\n",
        "    cos = 1 - F.cosine_similarity(z_pred, z_true, dim=-1).mean()\n",
        "    logits = csls_similarity(z_pred, z_true, k=10) / temperature   # ðŸ‘ˆ CSLS\n",
        "    labels = torch.arange(logits.size(0), device=z_pred.device)\n",
        "    contrast = F.cross_entropy(logits, labels)\n",
        "    with torch.no_grad():\n",
        "        sim_ranks = logits.topk(k=5, dim=1).indices\n",
        "    hard_mask = torch.zeros_like(logits); hard_mask.scatter_(1, sim_ranks, 1.0)\n",
        "    hard_neg = ((hard_mask * torch.exp(logits)).sum(1)).mean()\n",
        "    return alpha*mse + beta*cos + gamma*contrast + hard_neg_weight * hard_neg\n",
        "\n",
        "\n",
        "    return alpha*mse + beta*cos + gamma*contrast + hard_neg_weight * hard_neg\n",
        "\n",
        "def align_uniform_loss(z1, z2, alpha=2, lam=0.1):\n",
        "    # alignment: rendi vicini i positivi\n",
        "    align = (z1 - z2).pow(2).sum(1).pow(alpha / 2).mean()\n",
        "\n",
        "    # uniformity: calcolo manuale delle distanze\n",
        "    z1 = F.normalize(z1, dim=-1)\n",
        "    sim_matrix = z1 @ z1.T\n",
        "    sq_dist = 2 - 2 * sim_matrix  # ||x_i - x_j||^2 = 2(1 - cos)\n",
        "    uniform = torch.log(torch.exp(-lam * sq_dist).mean())\n",
        "    \n",
        "    return align + uniform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512cb034",
      "metadata": {},
      "outputs": [],
      "source": [
        "def supcon_loss(z_pred, img_ids, T=0.07):\n",
        "    z = F.normalize(z_pred, dim=-1)           # [B,D]\n",
        "    sim = z @ z.T / T                         # [B,B]\n",
        "    mask = (img_ids.unsqueeze(1) == img_ids.unsqueeze(0)).float()  # [B,B]\n",
        "    # rimuovi self\n",
        "    logits = sim - torch.eye(sim.size(0), device=sim.device) * 1e9\n",
        "    pos_mask = mask - torch.eye(sim.size(0), device=sim.device)\n",
        "    # loss per ancora i: - log (sum_j e^{s_ij} / sum_k e^{s_ik}) sui soli positivi\n",
        "    log_prob = logits - torch.logsumexp(logits, dim=1, keepdim=True)\n",
        "    # media solo sui positivi presenti\n",
        "    denom = pos_mask.sum(1).clamp_min(1.0)\n",
        "    loss = -(pos_mask * log_prob).sum(1) / denom\n",
        "    return loss.mean()\n",
        "\n",
        "def listnet_loss(z_pred, z_true, T=0.07):\n",
        "    \"\"\"ListNet-style loss: approssima un ranking globale tra i positivi.\"\"\"\n",
        "    zq = F.normalize(z_pred, dim=-1)\n",
        "    zg = F.normalize(z_true, dim=-1)\n",
        "    S = zq @ zg.T / T  # [B,B]\n",
        "    \n",
        "    # target \"ideale\": ogni riga dovrebbe assegnare tutta la probabilitÃ  alla colonna corretta\n",
        "    target = torch.eye(S.size(0), device=S.device)\n",
        "    \n",
        "    # distribuzioni predette (P) e target (Q)\n",
        "    P = torch.softmax(S, dim=1)\n",
        "    \n",
        "    # cross-entropy simile a KL div (batchmean)\n",
        "    return F.kl_div((P + 1e-8).log(), target, reduction='batchmean')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac46c8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def metric_reg(z_pred, z_text, k=30):\n",
        "    \"\"\"Forza coerenza locale tra spazi: se due caption sono simili in A, lo siano anche in B.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        sims_text = (z_text @ z_text.T)\n",
        "        idx = sims_text.topk(k, dim=1).indices\n",
        "        mask = torch.zeros_like(sims_text)\n",
        "        mask.scatter_(1, idx, 1.0)\n",
        "    sims_pred = (z_pred @ z_pred.T)\n",
        "    return ((mask * (sims_pred - sims_text))**2).mean()\n",
        "\n",
        "def relational_alignment_loss(z_pred, S_target, max_points=512):\n",
        "    \"\"\"\n",
        "    Encourage predicted space to preserve the target (VAE) relational geometry.\n",
        "    S_target: precomputed [M, M] similarity matrix (subset used automatically)\n",
        "    \"\"\"\n",
        "    N = z_pred.shape[0]\n",
        "    M = S_target.shape[0]\n",
        "\n",
        "    # Se batch piÃ¹ grande della matrice target, usa solo primi punti\n",
        "    if N > M:\n",
        "        z_pred = z_pred[:M]\n",
        "        N = M\n",
        "\n",
        "    # Estrai un sottoinsieme coerente da S_target (prima N righe e colonne)\n",
        "    S_sub = S_target[:N, :N].to(z_pred.device)\n",
        "\n",
        "    # Calcola struttura predetta\n",
        "    z_pred = F.normalize(z_pred, dim=-1)\n",
        "    S_pred = z_pred @ z_pred.T  # [N, N]\n",
        "\n",
        "    # MSE fra le due strutture\n",
        "    loss = F.mse_loss(S_pred, S_sub)\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f048007",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_geodesic_similarity(Y, scale=0.5, max_points=2000):\n",
        "    \"\"\"\n",
        "    Compute geodesic similarity with automatic sigma adaptation.\n",
        "\n",
        "    scale: multiplicative factor to adjust sigma relative to mean distance.\n",
        "           smaller -> more contrast, larger -> smoother.\n",
        "    \"\"\"\n",
        "    if len(Y) > max_points:\n",
        "        idx = torch.randperm(len(Y))[:max_points]\n",
        "        Y = Y[idx]\n",
        "\n",
        "    # Pairwise distances\n",
        "    dist2 = torch.cdist(Y, Y, p=2).pow(2)\n",
        "    mean_d2 = dist2.mean().item()\n",
        "    sigma = (mean_d2 ** 0.5) * scale   # e.g. 0.5 Ã— mean distance\n",
        "    print(f\"[auto-sigma] mean distanceÂ²={mean_d2:.4f}, sigma={sigma:.4f}\")\n",
        "\n",
        "    S = torch.exp(-dist2 / (2 * sigma**2))\n",
        "    S = S / S.max()\n",
        "    return S\n",
        "\n",
        "\n",
        "def local_centering(out, gallery, k=50):\n",
        "    \"\"\"Rimuove la media dei k-NN in 'gallery' per ridurre hubness.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        # SimilaritÃ  cosine normalizzata\n",
        "        sim = F.normalize(out, dim=-1) @ F.normalize(gallery, dim=-1).T  # [B, G]\n",
        "        idx = sim.topk(k, dim=1).indices                                 # [B, k]\n",
        "        neigh = gallery[idx]                                             # [B, k, D]\n",
        "        mu = neigh.mean(1)                                               # media locale\n",
        "    return out - mu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f083bfda",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_text_similarity(X, max_points=2000):\n",
        "    if len(X) > max_points:\n",
        "        idx = torch.randperm(len(X))[:max_points]\n",
        "        X = X[idx]\n",
        "    S = (F.cosine_similarity(X.unsqueeze(1), X.unsqueeze(0), dim=-1) + 1) / 2\n",
        "    return S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6883251f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def parallel_transport_loss(z_pred, z_text):\n",
        "    if z_text.size(0) < 2:\n",
        "        return torch.tensor(0.0, device=z_pred.device)\n",
        "    dz = z_text[1:] - z_text[:-1]\n",
        "    dp = z_pred[1:] - z_pred[:-1]\n",
        "    dz = F.normalize(dz, dim=-1)\n",
        "    dp = F.normalize(dp, dim=-1)\n",
        "    # riduci alla dimensione minima comune\n",
        "    d = min(dz.shape[-1], dp.shape[-1])\n",
        "    dz, dp = dz[..., :d], dp[..., :d]\n",
        "    return 1 - F.cosine_similarity(dp, dz, dim=-1).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30cbff24",
      "metadata": {
        "id": "30cbff24"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, device, epochs, lr, S_vae, y_train_full):\n",
        "    \"\"\"\n",
        "    Training completo con:\n",
        "    - SupCon + ListNet + Hybrid Loss\n",
        "    - Local Centering (anti-hubness)\n",
        "    - Relational + Uniform + Parallel Transport regularizers\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=5e-4,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(train_loader)\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for X_batch, y_batch, label_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            X_batch, y_batch, label_batch = (\n",
        "                X_batch.to(device),\n",
        "                y_batch.to(device),\n",
        "                label_batch.to(device)\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # === Forward ===\n",
        "            outputs = model(X_batch)\n",
        "\n",
        "            # âœ… Anti-hubness correction\n",
        "            outputs = local_centering(outputs, y_train_full, k=50)\n",
        "\n",
        "            # === Composite Loss ===\n",
        "            loss = (\n",
        "                0.5 * supcon_loss(outputs, label_batch)          # multi-positivi (ranking locale)\n",
        "              + 0.3 * hybrid_loss(outputs, y_batch)              # allineamento direzionale + contrastivo\n",
        "              + 0.1 * listnet_loss(outputs, y_batch)             # ranking listwise globale\n",
        "              + 0.1 * relational_alignment_loss(outputs, S_vae)  # coerenza geometrica\n",
        "              + 0.05 * align_uniform_loss(outputs, y_batch)      # distribuzione uniforme\n",
        "              + 0.05 * parallel_transport_loss(outputs, X_batch) # coerenza direzionale\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        val_loss, cos_sim, mse, contrastive = 0, 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch, label_batch in val_loader:\n",
        "                X_batch, y_batch, label_batch = (\n",
        "                    X_batch.to(device),\n",
        "                    y_batch.to(device),\n",
        "                    label_batch.to(device)\n",
        "                )\n",
        "                outputs = model(X_batch)\n",
        "                outputs = local_centering(outputs, y_train_full, k=50)  # stessa correzione anche in val\n",
        "\n",
        "                loss = (\n",
        "                    0.5 * supcon_loss(outputs, label_batch)\n",
        "                  + 0.3 * hybrid_loss(outputs, y_batch)\n",
        "                  + 0.1 * listnet_loss(outputs, y_batch)\n",
        "                  + 0.1 * relational_alignment_loss(outputs, S_vae)\n",
        "                  + 0.05 * align_uniform_loss(outputs, y_batch)\n",
        "                  + 0.05 * parallel_transport_loss(outputs, X_batch)\n",
        "                )\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # metriche base\n",
        "                cos_sim += F.cosine_similarity(outputs, y_batch, dim=-1).mean().item()\n",
        "                mse += F.mse_loss(outputs, y_batch).item()\n",
        "                sim = (F.normalize(outputs, dim=-1) @ F.normalize(y_batch, dim=-1).T)\n",
        "                pos = torch.arange(sim.size(0), device=device)\n",
        "                contrastive += F.cross_entropy(sim / 0.07, pos).item()\n",
        "\n",
        "        # === Averaging e logging ===\n",
        "        n_batches = len(val_loader)\n",
        "        val_loss /= n_batches\n",
        "        cos_sim /= n_batches\n",
        "        mse /= n_batches\n",
        "        contrastive /= n_batches\n",
        "\n",
        "        print(f\"Cosine={cos_sim:.4f}, MSE={mse:.4f}, Contrastive={contrastive:.4f}\")\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}: \"\n",
        "            f\"TrainLoss={train_loss:.4f}, ValLoss={val_loss:.4f} | \"\n",
        "            f\"MSE={mse:.4f}, Cosine={cos_sim:.4f}, Contrastive={contrastive:.4f}, \"\n",
        "            f\"MetricReg={metric_reg(outputs, X_batch):.4f}, \"\n",
        "            f\"AlignUniform={align_uniform_loss(outputs, y_batch):.4f}, \"\n",
        "            f\"Relational={relational_alignment_loss(outputs, S_vae):.4f}, \"\n",
        "            f\"PT={parallel_transport_loss(outputs, X_batch):.4f}\"\n",
        "        )\n",
        "\n",
        "        # === Checkpoint ===\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            print(f\"  âœ“ Saved best model (val_loss={val_loss:.6f})\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f486748",
      "metadata": {},
      "outputs": [],
      "source": [
        "def whiten(X, eps=1e-6):\n",
        "    X = X - X.mean(0, keepdim=True)\n",
        "    cov = (X.T @ X) / (X.shape[0] - 1)\n",
        "    eigvals, eigvecs = torch.linalg.eigh(cov)\n",
        "    \n",
        "    # Evita divisione per 0\n",
        "    eigvals = torch.clamp(eigvals, min=eps)\n",
        "    \n",
        "    W = eigvecs @ torch.diag(1.0 / torch.sqrt(eigvals)) @ eigvecs.T\n",
        "    return X @ W\n",
        "\n",
        "# SVD VERSION \n",
        "# def whiten(X, eps=1e-6):\n",
        "#     \"\"\"\n",
        "#     Whiten data matrix X using SVD decomposition (numerically stable).\n",
        "\n",
        "#     Args:\n",
        "#         X (torch.Tensor): input data of shape [N, D]\n",
        "#         eps (float): small constant for numerical stability\n",
        "#     Returns:\n",
        "#         torch.Tensor: whitened data of shape [N, D]\n",
        "#     \"\"\"\n",
        "#     # 1ï¸âƒ£ Center the data (remove mean)\n",
        "#     X = X - X.mean(0, keepdim=True)\n",
        "    \n",
        "#     # 2ï¸âƒ£ Compute SVD (does not require explicit covariance)\n",
        "#     # X = U * S * Vh, where columns of Vh are principal directions\n",
        "#     U, S, Vh = torch.linalg.svd(X, full_matrices=False)\n",
        "    \n",
        "#     # 3ï¸âƒ£ Scale by inverse sqrt of singular values\n",
        "#     # (equivalent to dividing by std in each PCA direction)\n",
        "#     X_white = (U @ torch.diag(1.0 / torch.sqrt(S + eps))) @ Vh\n",
        "    \n",
        "#     # 4ï¸âƒ£ Optional: normalize each vector to unit norm\n",
        "#     X_white = F.normalize(X_white, dim=-1)\n",
        "    \n",
        "#     return X_white"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75dc85c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75dc85c5",
        "outputId": "6a8c8b95-388e-45fe-cf09-ef67e502b392"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "train_data = load_data(\"data/train/train.npz\")\n",
        "X, y, label = prepare_train_data(train_data)\n",
        "\n",
        "X = F.normalize(X, dim=-1)\n",
        "y = F.normalize(y, dim=-1)\n",
        "\n",
        "# KMeans sui vettori NORMALIZZATI\n",
        "K = 64\n",
        "kmeans = KMeans(n_clusters=K, n_init=10, random_state=0).fit(X.cpu().numpy())\n",
        "centroids = torch.tensor(kmeans.cluster_centers_, dtype=X.dtype, device=DEVICE)\n",
        "\n",
        "# S_vae calcolata sui target normalizzati (coerente con S_pred)\n",
        "S_vae = compute_geodesic_similarity(y, scale=0.5)\n",
        "\n",
        "print(\"Computed S_vae with shape:\", S_vae.shape)\n",
        "plt.imshow(S_vae.cpu().numpy(), cmap='magma')\n",
        "plt.title(\"Approx. Geodesic Similarity (VAE latent space)\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# === Approximate Similarity in the Text Embedding Space ===\n",
        "S_text = compute_text_similarity(X, 512)\n",
        "print(\"Computed S_text with shape:\", S_text.shape)\n",
        "plt.imshow(S_text.cpu().numpy(), cmap='magma')\n",
        "plt.title(\"Text-space Similarity Matrix\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "DATASET_SIZE = len(X)\n",
        "# Split train/val\n",
        "# This is done only to measure generalization capabilities, you don't have to\n",
        "# use a validation set (though we encourage this)\n",
        "n_train = int(0.9 * len(X))\n",
        "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
        "TRAIN_SPLIT[:n_train] = 1\n",
        "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
        "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
        "\n",
        "print(label.shape)  # deve essere [N]\n",
        "print(label[TRAIN_SPLIT].shape)  # deve essere [n_train]\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train, label[TRAIN_SPLIT])\n",
        "val_dataset   = TensorDataset(X_val,   y_val,   label[~TRAIN_SPLIT])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963c0644",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "963c0644",
        "outputId": "2d9d58ff-e0af-4ab1-be34-71052cf00690"
      },
      "outputs": [],
      "source": [
        "model = LocalAffineMoE(\n",
        "    d_in=X.shape[1], d_out=y.shape[1], K=K, temp=0.07, centroids=centroids\n",
        ").to(DEVICE)\n",
        "\n",
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "y_train_full = y_train.to(DEVICE)\n",
        "\n",
        "# Training\n",
        "print(\"\\n3. Training...\")\n",
        "model = train_model(model, train_loader, val_loader, DEVICE, EPOCHS, LR, S_vae, y_train_full)\n",
        "\n",
        "\n",
        "# Load best model for evaluation\n",
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a533e6f",
      "metadata": {
        "id": "0a533e6f"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3319399",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_latent_space(model, X_val, y_val, device, method=\"pca\", n_samples=2000):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val = X_val[:n_samples].to(device)\n",
        "        y_val = y_val[:n_samples].to(device)\n",
        "        z_pred = model(X_val).cpu()\n",
        "        z_true = y_val.cpu()\n",
        "\n",
        "    if method == \"tsne\":\n",
        "        reducer = TSNE(n_components=2, perplexity=30, n_iter=1000, init=\"pca\")\n",
        "    else:\n",
        "        reducer = PCA(n_components=2)\n",
        "\n",
        "    z_pred_2d = reducer.fit_transform(z_pred)\n",
        "    z_true_2d = reducer.fit_transform(z_true)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(z_true_2d[:, 0], z_true_2d[:, 1], alpha=0.5, s=12, label=\"True (Target Space)\")\n",
        "    plt.scatter(z_pred_2d[:, 0], z_pred_2d[:, 1], alpha=0.5, s=12, label=\"Predicted (Mapped Space)\")\n",
        "    plt.title(f\"Latent Space Visualization ({method.upper()})\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_tsne(model, X_val, y_val, device, n_samples=2000, perplexity=30, seed=42):\n",
        "    \"\"\"\n",
        "    Visualizza le embedding predette e target nello spazio 2D con t-SNE.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val = X_val[:n_samples].to(device)\n",
        "        y_val = y_val[:n_samples].to(device)\n",
        "        z_pred = F.normalize(model(X_val), dim=-1).cpu().numpy()\n",
        "        z_true = F.normalize(y_val, dim=-1).cpu().numpy()\n",
        "\n",
        "    # Concatena per ottenere embedding congiunte\n",
        "    Z = np.concatenate([z_pred, z_true], axis=0)\n",
        "    labels = np.array([0] * len(z_pred) + [1] * len(z_true))  # 0=pred, 1=true\n",
        "\n",
        "    # Riduzione con t-SNE\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        perplexity=perplexity,\n",
        "        n_iter=1000,\n",
        "        init=\"pca\",\n",
        "        learning_rate=\"auto\",\n",
        "        random_state=seed,\n",
        "        verbose=1\n",
        "    )\n",
        "    Z_2d = tsne.fit_transform(Z)\n",
        "\n",
        "    # Split per colore\n",
        "    Z_pred_2d = Z_2d[labels == 0]\n",
        "    Z_true_2d = Z_2d[labels == 1]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(Z_true_2d[:, 0], Z_true_2d[:, 1],\n",
        "                alpha=0.45, s=18, c=\"#1f77b4\", label=\"True (target space)\")\n",
        "    plt.scatter(Z_pred_2d[:, 0], Z_pred_2d[:, 1],\n",
        "                alpha=0.45, s=18, c=\"#ff7f0e\", label=\"Predicted (mapped space)\")\n",
        "    plt.title(f\"t-SNE of Predicted vs True Embeddings ({n_samples} samples)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6322b10",
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def visualize_latent_space_3d(model, X_val, y_val, device, n_samples=2000, method=\"pca\"):\n",
        "    \"\"\"\n",
        "    Visualizza gli embedding predetti e target in 3D (interattivo) con Plotly.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val = X_val[:n_samples].to(device)\n",
        "        y_val = y_val[:n_samples].to(device)\n",
        "        z_pred = F.normalize(model(X_val), dim=-1).cpu().numpy()\n",
        "        z_true = F.normalize(y_val, dim=-1).cpu().numpy()\n",
        "\n",
        "    # Riduzione dimensionale (3D)\n",
        "    if method == \"tsne\":\n",
        "        from sklearn.manifold import TSNE\n",
        "        reducer = TSNE(n_components=3, perplexity=30, n_iter=1000, init=\"pca\", learning_rate=\"auto\")\n",
        "        Z_pred = reducer.fit_transform(z_pred)\n",
        "        Z_true = reducer.fit_transform(z_true)\n",
        "    else:\n",
        "        reducer = PCA(n_components=3)\n",
        "        Z_pred = reducer.fit_transform(z_pred)\n",
        "        Z_true = reducer.fit_transform(z_true)\n",
        "\n",
        "    # Creazione grafico interattivo\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Target space\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=Z_true[:,0], y=Z_true[:,1], z=Z_true[:,2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=3, color='royalblue', opacity=0.6),\n",
        "        name='True (Target Space)'\n",
        "    ))\n",
        "\n",
        "    # Predicted space\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=Z_pred[:,0], y=Z_pred[:,1], z=Z_pred[:,2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=3, color='orange', opacity=0.6),\n",
        "        name='Predicted (Mapped Space)'\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f\"3D Latent Space Visualization ({method.upper()})\",\n",
        "        scene=dict(\n",
        "            xaxis_title='Component 1',\n",
        "            yaxis_title='Component 2',\n",
        "            zaxis_title='Component 3'\n",
        "        ),\n",
        "        width=850,\n",
        "        height=700,\n",
        "        legend=dict(x=0.02, y=0.98)\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d060a9e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d060a9e8",
        "outputId": "3d91f177-0162-4da0-963a-93ba152a2b90"
      },
      "outputs": [],
      "source": [
        "from challenge.src.eval import visualize_retrieval\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "visualize_latent_space(model, X_val, y_val, DEVICE, method=\"pca\")\n",
        "visualize_tsne(model, X_val, y_val, DEVICE, n_samples=2000)\n",
        "\n",
        "\n",
        "visualize_latent_space_3d(model, X_val, y_val, DEVICE, n_samples=2000, method=\"pca\")\n",
        "visualize_latent_space_3d(model, X_val, y_val, DEVICE, n_samples=1000, method=\"tsne\")\n",
        "\n",
        "val_caption_text = train_data['captions/text'][~TRAIN_SPLIT]\n",
        "val_text_embd = X_val\n",
        "img_VAL_SPLIT = label[~TRAIN_SPLIT].sum(dim=0) > 0\n",
        "val_img_file = train_data['images/names'][img_VAL_SPLIT]\n",
        "val_img_embd = torch.from_numpy(train_data['images/embeddings'][img_VAL_SPLIT])\n",
        "val_label = np.nonzero(train_data['captions/label'][~TRAIN_SPLIT][:,img_VAL_SPLIT])[1]\n",
        "\n",
        "# Sample and visualize\n",
        "for i in range(5):\n",
        "    idx = np.random.randint(0, 100)\n",
        "    caption_embd = val_text_embd[idx]\n",
        "    caption_text = val_caption_text[idx]\n",
        "    gt_index = val_label[idx]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_embds = model(caption_embd.to(DEVICE)).cpu()\n",
        "\n",
        "        visualize_retrieval(\n",
        "            pred_embds,\n",
        "            gt_index,\n",
        "            val_img_file,\n",
        "            caption_text, val_img_embd, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09d07798",
      "metadata": {
        "id": "09d07798"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156b49a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "156b49a0",
        "outputId": "dc5070d3-f3a7-427b-e98c-76cbb957a4dd"
      },
      "outputs": [],
      "source": [
        "test_data = load_data(\"data/test/test.clean.npz\")\n",
        "\n",
        "test_embds = test_data['captions/embeddings']\n",
        "test_embds = torch.from_numpy(test_embds).float()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_embds = model(test_embds.to(DEVICE)).cpu()\n",
        "\n",
        "submission = generate_submission(test_data['captions/ids'], pred_embds, 'submission.csv')\n",
        "print(f\"Model saved to: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mVsCiM2MmLKE",
      "metadata": {
        "id": "mVsCiM2MmLKE"
      },
      "outputs": [],
      "source": [
        "# --- Evaluate retrieval performance ---\n",
        "from challenge.src.eval import evaluate_retrieval  # adattalo al tuo path\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_pred_val = model(X_val.to(DEVICE)).cpu()\n",
        "\n",
        "# ground truth embeddings (target images)\n",
        "z_img_val = y_val.cpu()\n",
        "gt_indices = np.arange(len(z_img_val))  # ogni caption ha la sua immagine target\n",
        "\n",
        "results = evaluate_retrieval(\n",
        "    translated_embd=z_pred_val,\n",
        "    image_embd=z_img_val,\n",
        "    gt_indices=gt_indices,\n",
        "    max_indices=50,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "print(\"\\n=== Retrieval evaluation (validation set) ===\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k:>12}: {v:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
