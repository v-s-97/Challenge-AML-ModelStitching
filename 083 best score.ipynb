{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26bf6fcc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from challenge.src.common import load_data, prepare_train_data, generate_submission\n",
    "from challenge.src.eval import evaluate_retrieval, visualize_retrieval\n",
    "# ==== Config ====\n",
    "MODEL_PATH = \"models/maxmatch_adapter_k4_sinkhorn.pth\"\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# MaxMatch / Set-based\n",
    "K_SLOTS = 6                # K moderato (4–6) per efficienza\n",
    "SLOT_DROPOUT_P = 0.1       # piccolo dropout per diversità iniziale\n",
    "# Sinkhorn (soft assignment)\n",
    "SINKHORN_ITERS = 10       # 5–15 è tipico\n",
    "SINKHORN_TAU = 0.20         # temperatura/entropic reg.; più piccolo = più \"dura\"\n",
    "DETACH_ASSIGNMENT = True   # come nel codice MaxMatch: stop-grad sull’assegnamento\n",
    "\n",
    "# --- MaxMatch hyper ---\n",
    "SCALE_S    = 0.5    # \"s\" (scala esponenziale per ISDL/GDL)\n",
    "DELTA_1    = 0.20   # margine Triplet su S_H (δ1)\n",
    "DELTA_2    = 0.70   # margine GDL (δ2)\n",
    "DELTA_3    = 0.70   # margine ISDL (δ3)\n",
    "\n",
    "LAMBDA_ISDL = 0.25  # peso ISDL (intra-set)\n",
    "LAMBDA_GDL  = 0.25  # peso GDL  (globale)\n",
    "LAMBDA_MMD  = 0.01  # opzionale\n",
    "LAMBDA_DIV  = 0.01  # opzionale\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Head set-based: t(1024) -> S_T (K x 1536)\n",
    "# ============================================================\n",
    "class SetPredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Converte un embedding testuale globale in K slot nello spazio visivo (1536).\n",
    "    - K query vettori learnable (inizializzati nel target space).\n",
    "    - Proiezione testo -> spazio visivo + gating per diversificare contributi.\n",
    "    - Piccolo dropout per evitare collasso precoce.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_text=1024, d_vis=1536, K=4, hidden=2048, slot_dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.d_vis = d_vis\n",
    "\n",
    "        self.text_to_vis = nn.Sequential(\n",
    "            nn.Linear(d_text, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, d_vis),\n",
    "        )\n",
    "\n",
    "        self.slot_queries = nn.Parameter(torch.randn(K, d_vis) * 0.02)\n",
    "        self.gate_per_slot = nn.Linear(d_text, K)\n",
    "        self.delta_per_slot = nn.Linear(d_text, K * d_vis)\n",
    "\n",
    "        self.ln_slots = nn.LayerNorm(d_vis)\n",
    "        self.ln_text  = nn.LayerNorm(d_vis)\n",
    "        self.dropout = nn.Dropout(p=slot_dropout_p)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        B = t.size(0)\n",
    "        t_vis = self.text_to_vis(t)                 # (B, d_vis)\n",
    "        t_vis = self.ln_text(t_vis)\n",
    "\n",
    "        gate  = torch.sigmoid(self.gate_per_slot(t))      # (B, K)\n",
    "        delta = self.delta_per_slot(t).view(B, self.K, self.d_vis)\n",
    "\n",
    "        Q = self.slot_queries.unsqueeze(0).expand(B, -1, -1)  # (B, K, d_vis)\n",
    "        t_vis_exp = t_vis.unsqueeze(1).expand(-1, self.K, -1)\n",
    "        gate_exp  = gate.unsqueeze(-1)\n",
    "\n",
    "        # --- residui (pre-fusione globale) ---\n",
    "        R = Q + gate_exp * t_vis_exp + delta               # (B, K, d_vis)\n",
    "        R = self.ln_slots(R)\n",
    "        R = self.dropout(R)\n",
    "        E_T = F.normalize(R, dim=-1)                       # residui normalizzati (per ISDL)\n",
    "\n",
    "        # --- fusione globale per scoring ---\n",
    "        S_T = E_T + F.normalize(t_vis, dim=-1).unsqueeze(1)\n",
    "        S_T = F.normalize(S_T, dim=-1)\n",
    "        return S_T, E_T, F.normalize(t_vis, dim=-1)\n",
    "# ============================================================\n",
    "# 2) Sinkhorn matching (soft, entropic-regularized doubly-stochastic)\n",
    "#    sim -> P ~ doubly-stochastic, poi S_H = <P, sim>/K\n",
    "# ============================================================\n",
    "def sinkhorn_logspace(log_K: torch.Tensor, iters: int = 10) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Log-space Sinkhorn per stabilità numerica.\n",
    "    log_K: (B, K, K) log-kernel (logits pre-softmax)\n",
    "    Ritorna log_P: (B, K, K) ~ log matrix bistocastica\n",
    "    \"\"\"\n",
    "    B, K, _ = log_K.shape\n",
    "    log_u = torch.zeros(B, K, device=log_K.device)\n",
    "    log_v = torch.zeros(B, K, device=log_K.device)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        # normalizza righe\n",
    "        log_u = -torch.logsumexp(log_K + log_v.unsqueeze(1), dim=2)\n",
    "        # normalizza colonne\n",
    "        log_v = -torch.logsumexp(log_K + log_u.unsqueeze(2), dim=1)\n",
    "\n",
    "    log_P = log_K + log_u.unsqueeze(2) + log_v.unsqueeze(1)\n",
    "    return log_P\n",
    "\n",
    "def sinkhorn_assignment(sim: torch.Tensor, tau: float = 0.1, iters: int = 10,\n",
    "                        detach_input: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    sim: (B, K, K) similarità coseno.\n",
    "    Costruisce kernel K_ij = exp(sim_ij / tau), applica Sinkhorn per ottenere P ~ bistocastica.\n",
    "    Se detach_input=True, rimuove il gradiente dalla matrice di sim nel calcolo dell'assegnamento,\n",
    "    replicando lo schema \"stop-grad\" usato in MaxMatch per la parte di matching.\n",
    "    \"\"\"\n",
    "    if detach_input:\n",
    "        sim = sim.detach()\n",
    "    log_K = sim / max(tau, 1e-6)\n",
    "    log_P = sinkhorn_logspace(log_K, iters=iters)\n",
    "    P = torch.exp(log_P)  # (B, K, K), righe/colonne ~ 1\n",
    "    return P\n",
    "\n",
    "def cosine_matrix(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    # (B, K, D) x (B, D, K) -> (B, K, K)\n",
    "    return torch.matmul(A, B.transpose(-1, -2))\n",
    "\n",
    "def s_h_maxmatch_sinkhorn(\n",
    "    S_T: torch.Tensor, V: torch.Tensor, *,\n",
    "    tau: float, iters: int, detach: bool\n",
    ") -> torch.Tensor:\n",
    "    B, K, D = S_T.shape\n",
    "    Vn = F.normalize(V, dim=-1)\n",
    "    S_V = Vn.unsqueeze(1).expand(-1, K, -1)        # (B, K, D)\n",
    "    sims = torch.matmul(S_T, S_V.transpose(-1, -2))# (B, K, K)\n",
    "    P = sinkhorn_assignment(sims, tau=tau, iters=iters, detach_input=detach)\n",
    "    return (P * sims).sum(dim=(1, 2)) / K\n",
    "    \n",
    "USE_DEGENERATE_SH = True  # True: rapido e identico nel caso colonne uguali\n",
    "\n",
    "def s_h_singleton_target(S_T: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    # S_T: (B,K,D), V: (B,D)\n",
    "    Vn = F.normalize(V, dim=-1)\n",
    "    sims = torch.einsum('bkd,bd->bk', S_T, Vn)  # (B,K)\n",
    "    return sims.mean(dim=1)                      # (B,)\n",
    "\n",
    "def s_h(S_T: torch.Tensor, V: torch.Tensor, *, tau, iters, detach) -> torch.Tensor:\n",
    "    if USE_DEGENERATE_SH:\n",
    "        return s_h_singleton_target(S_T, V)\n",
    "    else:\n",
    "        return s_h_maxmatch_sinkhorn(S_T, V, tau=tau, iters=iters, detach=detach)\n",
    "\n",
    "\n",
    "\n",
    "def triplet_maxmatch_sh(S_T, V, delta1, *, tau, iters, detach):\n",
    "    B, K, D = S_T.shape\n",
    "    s_pos = s_h_maxmatch_sinkhorn(S_T, V, tau=tau, iters=iters, detach=detach)  # (B,)\n",
    "    V_all = F.normalize(V, dim=-1)\n",
    "\n",
    "    max_negs = []\n",
    "    CH = 64  # blocco sicuro; puoi alzare/abbassare in base alla GPU\n",
    "    for start in range(0, B, CH):\n",
    "        end = min(B, start + CH)\n",
    "        S_blk = S_T[start:end]                                   # (ch,K,D)\n",
    "\n",
    "        # confronta ogni S_blk[i] con tutte le immagini del batch (B)\n",
    "        S_exp = S_blk.unsqueeze(1).expand(end - start, B, K, D).reshape((end - start) * B, K, D)\n",
    "        V_exp = V_all.unsqueeze(0).expand(end - start, B, D).reshape((end - start) * B, D)\n",
    "\n",
    "        s_blk = s_h(S_exp, V_exp, tau=tau, iters=iters, detach=detach)\n",
    "\n",
    "        s_blk = s_blk.view(end - start, B)                       # (ch, B)\n",
    "\n",
    "        # maschera SOLO il positivo per riga: colonna (start + r)\n",
    "        rows = torch.arange(end - start, device=S_T.device)\n",
    "        cols = torch.arange(start, end, device=S_T.device)\n",
    "        s_blk[rows, cols] = float('-inf')\n",
    "\n",
    "        max_negs.append(s_blk.max(dim=1).values)                 # (ch,)\n",
    "\n",
    "    s_neg = torch.cat(max_negs, dim=0)                           # (B,)\n",
    "    return F.relu(delta1 + s_neg - s_pos).mean()\n",
    "\n",
    "# ============================================================\n",
    "# 3) ISDL – Intra-Set Diversity Loss (Alomari 2025)\n",
    "#     Minimizza similarità intra-slot (promuove diversità)\n",
    "# ============================================================\n",
    "def isdl_intra_set_diversity_exp(S_T: torch.Tensor, s: float, delta3: float) -> torch.Tensor:\n",
    "    B, K, D = S_T.shape\n",
    "    C = torch.matmul(S_T, S_T.transpose(-1, -2)).clamp(-1, 1)  # (B,K,K)\n",
    "    mask = ~torch.eye(K, device=S_T.device, dtype=torch.bool).unsqueeze(0).expand(B, K, K)\n",
    "    C_off = C[mask]  # (B*K*(K-1),)\n",
    "    loss = torch.exp(s * (C_off - delta3)).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_data = load_data(\"data/train/train.npz\")\n",
    "X, y, label = prepare_train_data(train_data)\n",
    "DATASET_SIZE = len(X)\n",
    "# Split train/val\n",
    "# This is done only to measure generalization capabilities, you don't have to\n",
    "# use a validation set (though we encourage this)\n",
    "n_train = int(0.9 * len(X))\n",
    "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
    "TRAIN_SPLIT[:n_train] = 1\n",
    "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
    "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "y_train.shape[-1], X_val.shape[-1]\n",
    "# ============================================================\n",
    "# 4) GDL – Global Discriminative Loss (Alomari 2025)\n",
    "#     Rafforza separazione vs negativi di batch (ranking)\n",
    "# ============================================================\n",
    "def gdl_global_discriminative_true(S_T, t_vis_norm, s: float, delta2: float):\n",
    "    \"\"\"\n",
    "    GDL: penalizza allineamento slot ↔ globale (stessa modalità/spazio),\n",
    "    spingendo gli slot a non collassare sul globale.\n",
    "    \"\"\"\n",
    "    # sim per-slot col globale: (B,K)\n",
    "    sims = torch.einsum('bkd,bd->bk', S_T, t_vis_norm).clamp(-1,1)\n",
    "    return torch.exp(s * (sims - delta2)).mean()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Diagnostica: log-varianza intra-slot\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def slot_log_variance(S_T: torch.Tensor) -> float:\n",
    "    var_fd = S_T.var(dim=1, unbiased=False)   # (B, D)\n",
    "    var_mean = var_fd.mean(dim=1).mean().clamp_min(1e-8)\n",
    "    return float(torch.log(var_mean).item())\n",
    "def curriculum_params(epoch_1based: int):\n",
    "    if epoch_1based <= 10:\n",
    "        return {\n",
    "            \"lambda_isdl\": LAMBDA_ISDL,\n",
    "            \"lambda_gdl\":  LAMBDA_GDL,\n",
    "            \"tau\":          SINKHORN_TAU,\n",
    "            \"iters\":        SINKHORN_ITERS,\n",
    "            \"delta1\":       DELTA_1,\n",
    "            \"delta2\":       DELTA_2,\n",
    "            \"delta3\":       DELTA_3,\n",
    "            \"scale_s\":      SCALE_S,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"lambda_isdl\": LAMBDA_ISDL,\n",
    "            \"lambda_gdl\":  LAMBDA_GDL + 0.05,\n",
    "            \"tau\":          max(1e-3, SINKHORN_TAU - 0.01),\n",
    "            \"iters\":        SINKHORN_ITERS + 2,\n",
    "            \"delta1\":       DELTA_1,\n",
    "            \"delta2\":       DELTA_2,\n",
    "            \"delta3\":       DELTA_3,\n",
    "            \"scale_s\":      SCALE_S,\n",
    "        }\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_offdiag_cos(E_Tn):\n",
    "    K = E_Tn.size(1)\n",
    "    C = torch.matmul(E_Tn, E_Tn.transpose(-1,-2))          # (B,K,K)\n",
    "    off = C - torch.eye(K, device=E_Tn.device).unsqueeze(0)\n",
    "    denom = max(K*(K-1), 1)\n",
    "    return float(off.abs().sum(dim=(1,2)).mean().item() / denom)\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=1.0):\n",
    "    x2 = (x*x).sum(dim=1, keepdim=True)\n",
    "    y2 = (y*y).sum(dim=1, keepdim=True)\n",
    "    xy = x @ y.t()\n",
    "    dist = x2 - 2*xy + y2.t()\n",
    "    return torch.exp(-dist / (2*sigma**2))\n",
    "\n",
    "def mmd_rbf(x, y, sigma=1.0):\n",
    "    Kxx = gaussian_kernel(x, x, sigma).mean()\n",
    "    Kyy = gaussian_kernel(y, y, sigma).mean()\n",
    "    Kxy = gaussian_kernel(x, y, sigma).mean()\n",
    "    return Kxx + Kyy - 2*Kxy\n",
    "\n",
    "def diversity_regularizer_exp(E: torch.Tensor, s: float = 1.0):\n",
    "    \"\"\"\n",
    "    Accetta:\n",
    "      - E: (B,K,D)  -> viene flattenato a (B*K, D)\n",
    "      - E: (N,D)    -> usato così com'è\n",
    "    \"\"\"\n",
    "    if E.dim() == 3:\n",
    "        B, K, D = E.shape\n",
    "        E = E.reshape(B * K, D)\n",
    "    elif E.dim() == 2:\n",
    "        D = E.size(1)\n",
    "    else:\n",
    "        raise ValueError(f\"diversity_regularizer_exp: atteso 2D/3D, trovato {tuple(E.shape)}\")\n",
    "\n",
    "    C = (E @ E.t()).clamp(-1, 1)  # (N,N)\n",
    "    mask = ~torch.eye(E.size(0), device=E.device, dtype=torch.bool)\n",
    "    C_off = C[mask]\n",
    "    return torch.exp(-2.0 * (1 - C_off)).mean()\n",
    "\n",
    "\n",
    "def subsample_rows(X: torch.Tensor, max_n: int):\n",
    "    n = X.size(0)\n",
    "    if n <= max_n: return X\n",
    "    idx = torch.randint(0, n, (max_n,), device=X.device)\n",
    "    return X.index_select(0, idx)\n",
    "\n",
    "# Subsample per evitare O(n^2) pieno\n",
    "AGG_MAX = 256      # righe per MMD\n",
    "RES_MAX = 512      # righe per DIV (BK ~ B*K)\n",
    "def train_model(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                device: torch.device,\n",
    "                epochs: int,\n",
    "                lr: float,\n",
    "                logvar_warm_epochs: int = 3) -> nn.Module:\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "    best_val = -1e9\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_s_h_sum, train_batches = 0.0, 0\n",
    "        logvar_vals, offdiag_vals = [], []\n",
    "\n",
    "        for Xb, Yb in tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{epochs}\"):\n",
    "            cur = curriculum_params(epoch)\n",
    "\n",
    "            Xb = Xb.to(device, non_blocking=True)\n",
    "            Yb = Yb.to(device, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "                # --- forward ---\n",
    "                S_T, E_T, t_vis_n = model(Xb)      # S_T: scoring; E_T: residui (ISDL); t_vis_n: globale normalizzato\n",
    "                E_Tn = F.normalize(E_T, dim=-1)\n",
    "\n",
    "                # --- diagnostica ---\n",
    "                if epoch <= logvar_warm_epochs:\n",
    "                    logvar_vals.append(slot_log_variance(S_T))\n",
    "                offdiag_vals.append(mean_offdiag_cos(E_Tn))\n",
    "\n",
    "                # --- s_H (positivo) per logging: forma chiusa (rapida) ---\n",
    "                s_pos = s_h(\n",
    "                    S_T, Yb,\n",
    "                    tau=cur[\"tau\"], iters=cur[\"iters\"], detach=DETACH_ASSIGNMENT\n",
    "                )  # (B,)\n",
    "\n",
    "                # --- loss principali ---\n",
    "                # 1) Triplet su S_H (hardest-neg su batch) – usa s_h \"wrapper\" all'interno\n",
    "                loss_tri = triplet_maxmatch_sh(\n",
    "                    S_T, Yb, cur[\"delta1\"],\n",
    "                    tau=cur[\"tau\"], iters=cur[\"iters\"], detach=DETACH_ASSIGNMENT\n",
    "                )\n",
    "\n",
    "                # 2) ISDL (intra-set, exp con margine)\n",
    "                loss_isdl = isdl_intra_set_diversity_exp(\n",
    "                    S_T, s=cur[\"scale_s\"], delta3=cur[\"delta3\"]\n",
    "                )\n",
    "\n",
    "                # 3) GDL (slot vs globale t_vis nello stesso spazio)\n",
    "                loss_gdl = gdl_global_discriminative_true(\n",
    "                    S_T, t_vis_n, s=cur[\"scale_s\"], delta2=cur[\"delta2\"]\n",
    "                )\n",
    "\n",
    "                # 4) Opzionali: MMD (media slot ↔ target) e Diversità residui con subsample\n",
    "                agg_mean = F.normalize(S_T.mean(dim=1), dim=-1)  # (B,D)\n",
    "                agg_mean_ss = subsample_rows(agg_mean.detach(), AGG_MAX)\n",
    "                Yb_ss       = subsample_rows(F.normalize(Yb, dim=-1).detach(), AGG_MAX)\n",
    "                loss_mmd = mmd_rbf(agg_mean_ss, Yb_ss, sigma=1.0)\n",
    "\n",
    "                E_flat_ss = subsample_rows(E_Tn.reshape(-1, E_Tn.size(-1)), RES_MAX)\n",
    "                loss_div  = diversity_regularizer_exp(E_flat_ss, s=1.0)\n",
    "\n",
    "                loss = (loss_tri\n",
    "                        + cur[\"lambda_isdl\"] * loss_isdl\n",
    "                        + cur[\"lambda_gdl\"]  * loss_gdl\n",
    "                        + LAMBDA_MMD * loss_mmd\n",
    "                        + LAMBDA_DIV * loss_div)\n",
    "\n",
    "            # --- backward + step (AMP) ---\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "\n",
    "            # --- logging train ---\n",
    "            train_s_h_sum += float(s_pos.mean().item())\n",
    "            train_batches += 1\n",
    "\n",
    "        train_s_h_avg = train_s_h_sum / max(train_batches, 1)\n",
    "\n",
    "        # ===== Validation: S_H medio come surrogato, versione rapida =====\n",
    "        model.eval()\n",
    "        val_s_h_sum, val_batches = 0.0, 0\n",
    "        cur_val = curriculum_params(epoch)\n",
    "        with torch.no_grad():\n",
    "            for Xb, Yb in DataLoader(val_loader.dataset, batch_size=BATCH_SIZE, shuffle=False):\n",
    "                Xb = Xb.to(device, non_blocking=True)\n",
    "                Yb = Yb.to(device, non_blocking=True)\n",
    "                S_T, _, _ = model(Xb)\n",
    "                s_h_val = s_h(\n",
    "                    S_T, Yb,\n",
    "                    tau=cur_val[\"tau\"], iters=cur_val[\"iters\"], detach=DETACH_ASSIGNMENT\n",
    "                )\n",
    "                val_s_h_sum += float(s_h_val.mean().item())\n",
    "                val_batches += 1\n",
    "\n",
    "        val_s_h_avg = val_s_h_sum / max(val_batches, 1)\n",
    "\n",
    "        logvar_text = \"\"\n",
    "        if len(logvar_vals) > 0:\n",
    "            logvar_epoch = sum(logvar_vals) / len(logvar_vals)\n",
    "            logvar_text = f\" | log-var(S_T): {logvar_epoch:.2f}\"\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d}: train S_H={train_s_h_avg:.4f} | \"\n",
    "            f\"val S_H={val_s_h_avg:.4f} | last_loss={float(loss.item()):.4f} | \"\n",
    "            f\"offdiag(E_T): {sum(offdiag_vals)/len(offdiag_vals):.4f}{logvar_text}\"\n",
    "        )\n",
    "\n",
    "        if val_s_h_avg > best_val:\n",
    "            best_val = val_s_h_avg\n",
    "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"  ✓ Saved best (val S_H={val_s_h_avg:.4f})\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# 8) Istanziazione e training (coerente con MaxMatch + diagnostiche)\n",
    "# ============================================================\n",
    "model = SetPredictionHead(\n",
    "    d_text=X.shape[1],\n",
    "    d_vis=y.shape[1],\n",
    "    K=K_SLOTS,\n",
    "    hidden=2048,\n",
    "    slot_dropout_p=SLOT_DROPOUT_P\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\n3. Training (MaxMatch + Sinkhorn + ISDL + GDL, con curriculum su τ/iters e monitor ISDL/entropia)...\")\n",
    "\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR\n",
    ")\n",
    "\n",
    "\n",
    "# Carica best e imposta eval\n",
    "state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "# ============================================================\n",
    "# 9) Aggregazione slot -> 1 embedding (submission compatibile)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def aggregate_slots(S_T: torch.Tensor, V_ref: torch.Tensor | None = None, mode: str = \"mean\"):\n",
    "    \"\"\"\n",
    "    S_T:   (B, K, D)  slots già L2-normalizzati\n",
    "    V_ref: (B, D)     opzionale, solo per mode=\"winner\" (deve essere batch-aligned!)\n",
    "    Ritorna: (B, D) L2-normalizzato\n",
    "    \"\"\"\n",
    "    # Caso (B,K,D) -> assicura batch-dim\n",
    "    assert S_T.dim() == 3, f\"aggregate_slots: atteso (B,K,D), trovato {tuple(S_T.shape)}\"\n",
    "    if mode == \"mean\" or V_ref is None:\n",
    "        out = S_T.mean(dim=1)                  # (B, D)\n",
    "        return F.normalize(out, dim=-1)\n",
    "\n",
    "    # winner-slot richiede un riferimento allineato per ogni sample\n",
    "    assert V_ref is not None, \"mode='winner' richiede V_ref\"\n",
    "    assert V_ref.dim() == 2 and V_ref.size(0) == S_T.size(0), \\\n",
    "        f\"V_ref shape mismatch: atteso (B,D) con B={S_T.size(0)}, trovato {tuple(V_ref.shape)}\"\n",
    "\n",
    "    Vn = F.normalize(V_ref, dim=-1)            # (B, D)\n",
    "    # Similarità per-sample: per ogni b, sim di ciascuno slot verso V_ref[b]\n",
    "    sims = torch.einsum('bkd,bd->bk', S_T, Vn) # (B, K)\n",
    "    idx  = sims.argmax(dim=1)                  # (B,)\n",
    "    out  = S_T[torch.arange(S_T.size(0), device=S_T.device), idx, :]  # (B, D)\n",
    "    return F.normalize(out, dim=-1)\n",
    "\n",
    "\n",
    "# Visualizza qualche retrieval\n",
    "import numpy as np\n",
    "val_caption_text = train_data['captions/text'][~TRAIN_SPLIT]\n",
    "val_text_embd = X_val\n",
    "img_VAL_SPLIT = label[~TRAIN_SPLIT].sum(dim=0) > 0\n",
    "val_img_file = train_data['images/names'][img_VAL_SPLIT]\n",
    "val_img_embd = torch.from_numpy(train_data['images/embeddings'][img_VAL_SPLIT])\n",
    "val_label = np.nonzero(train_data['captions/label'][~TRAIN_SPLIT][:, img_VAL_SPLIT])[1]\n",
    "\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(0, min(100, len(val_text_embd)))\n",
    "    caption_embd = val_text_embd[idx:idx+1].to(DEVICE)\n",
    "    caption_text = val_caption_text[idx]\n",
    "    gt_index = val_label[idx]\n",
    "    with torch.no_grad():\n",
    "        S_T, _, _ = model(caption_embd)\n",
    "        pred_embd = aggregate_slots(S_T, mode=\"mean\").cpu().squeeze(0)\n",
    "        visualize_retrieval(pred_embd, gt_index, val_img_file, caption_text, val_img_embd, k=5)\n",
    "\n",
    "# Retrieval eval (compatibile con funzione originale)\n",
    "with torch.no_grad():\n",
    "    preds_val = []\n",
    "    for Xb in DataLoader(val_dataset.tensors[0], batch_size=BATCH_SIZE):\n",
    "        Xb = Xb.to(DEVICE)\n",
    "        S_Tb, _, _ = model(Xb)\n",
    "        Eb = aggregate_slots(S_Tb, mode=\"mean\")\n",
    "        preds_val.append(Eb.cpu())\n",
    "    z_pred_val = torch.cat(preds_val, dim=0)\n",
    "\n",
    "n_caps = z_pred_val.shape[0]\n",
    "n_imgs = val_img_embd.shape[0]\n",
    "if n_imgs < n_caps:\n",
    "    n_repeat = int(np.ceil(n_caps / n_imgs))\n",
    "    image_embd_equal = val_img_embd.repeat((n_repeat, 1))[:n_caps]\n",
    "elif n_imgs > n_caps:\n",
    "    image_embd_equal = val_img_embd[:n_caps]\n",
    "else:\n",
    "    image_embd_equal = val_img_embd\n",
    "\n",
    "gt_equal = val_label[:n_caps]\n",
    "\n",
    "image_embd_equal = F.normalize(image_embd_equal, dim=-1)\n",
    "z_pred_val       = F.normalize(z_pred_val, dim=-1)\n",
    "\n",
    "results = evaluate_retrieval(\n",
    "    translated_embd=z_pred_val,\n",
    "    image_embd=image_embd_equal,\n",
    "    gt_indices=gt_equal,\n",
    "    max_indices=min(99, image_embd_equal.shape[0]),\n",
    "    batch_size=100\n",
    ")\n",
    "print(\"\\n=== Validation Retrieval Metrics (MaxMatch+ISDL+GDL, agg='mean') ===\")\n",
    "for name, value in results.items():\n",
    "    print(f\"{name:15s}: {value:.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 1) Predizioni (CPU, L2-normalizzate)\n",
    "    preds_val_mean, preds_val_win = [], []\n",
    "    for Xb, Yb in DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False):\n",
    "        Xb = Xb.to(DEVICE); Yb = Yb.to(DEVICE)\n",
    "        S_Tb, _, _ = model(Xb)                              # (B,K,D)\n",
    "        Eb_mean = aggregate_slots(S_Tb, mode=\"mean\")     # (B,D)\n",
    "        Eb_win  = aggregate_slots(S_Tb, V_ref=Yb, mode=\"winner\")\n",
    "        preds_val_mean.append(Eb_mean.cpu())\n",
    "        preds_val_win.append(Eb_win.cpu())\n",
    "\n",
    "    Z_mean = F.normalize(torch.cat(preds_val_mean, dim=0), dim=-1).cpu()  # (Nq,D)\n",
    "    Z_win  = F.normalize(torch.cat(preds_val_win,  dim=0), dim=-1).cpu()  # (Nq,D)\n",
    "\n",
    "# 2) Equalizza a Nq e **RIORDINA** le immagini secondo gt (allineamento 1-a-1 nel batch)\n",
    "nq        = Z_mean.size(0)\n",
    "gt_equal  = np.asarray(val_label[:nq], dtype=np.int64)         # (Nq,)\n",
    "IMG_ALL_n = F.normalize(val_img_embd, dim=-1).cpu()            # (Nimg,D)\n",
    "# chiave: per la caption i-esima, l’immagine nel batch deve essere la sua GT\n",
    "IMG_EQ    = IMG_ALL_n[torch.from_numpy(gt_equal)]              # (Nq,D) riordinata\n",
    "\n",
    "# 3) Scegli un batch “sicuro” che divide Nq e imposta k ≤ batch\n",
    "def best_divisor(n, cap=100):\n",
    "    for d in [cap, 64, 50, 40, 32, 25, 20, 16, 10, 8, 5, 4, 2, 1]:\n",
    "        if d <= n and n % d == 0:\n",
    "            return d\n",
    "    # fallback\n",
    "    for d in range(min(cap, n), 0, -1):\n",
    "        if n % d == 0:\n",
    "            return d\n",
    "    return 1\n",
    "\n",
    "safe_batch = best_divisor(nq, 100)\n",
    "safe_k     = min(50, safe_batch)   # <= SEMPRE ≤ batch del prof\n",
    "\n",
    "# 4) Valutazione (mean e winner) – tutto CPU, k e batch sicuri, immagini allineate\n",
    "res_mean = evaluate_retrieval(\n",
    "    translated_embd=Z_mean, \n",
    "    image_embd=IMG_EQ, \n",
    "    gt_indices=gt_equal,\n",
    "    max_indices=safe_k,\n",
    "    batch_size=safe_batch\n",
    ")\n",
    "res_win = evaluate_retrieval(\n",
    "    translated_embd=Z_win, \n",
    "    image_embd=IMG_EQ, \n",
    "    gt_indices=gt_equal,\n",
    "    max_indices=safe_k,\n",
    "    batch_size=safe_batch\n",
    ")\n",
    "\n",
    "print(\"\\n=== agg='mean' ===\")\n",
    "for k,v in res_mean.items(): print(f\"{k:15s}: {v:.4f}\")\n",
    "print(\"\\n=== agg='winner' ===\")\n",
    "for k,v in res_win.items():  print(f\"{k:15s}: {v:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 10) Submission (robusta)\n",
    "# ============================================================\n",
    "test_data   = load_data(\"data/test/test.clean.npz\")\n",
    "test_ids    = test_data['captions/ids']\n",
    "test_embds  = torch.from_numpy(test_data['captions/embeddings']).float()  # (N,D_t)\n",
    "\n",
    "pred_chunks = []\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    for Xb in DataLoader(test_embds, batch_size=BATCH_SIZE, shuffle=False):\n",
    "        Xb = Xb.to(DEVICE, non_blocking=False)\n",
    "        S_Tb, _, _ = model(Xb)                    # (B,K,D)\n",
    "        Eb = aggregate_slots(S_Tb, mode=\"mean\")# (B,D) già L2-normalizzato\n",
    "        pred_chunks.append(Eb.cpu().to(torch.float32))\n",
    "\n",
    "pred_embds_test = torch.cat(pred_chunks, dim=0)  # (N,D) CPU float32\n",
    "\n",
    "# sanity checks\n",
    "assert pred_embds_test.ndim == 2, f\"Got shape {tuple(pred_embds_test.shape)}\"\n",
    "assert len(test_ids) == pred_embds_test.size(0), \\\n",
    "    f\"ids({len(test_ids)}) != preds({pred_embds_test.size(0)})\"\n",
    "\n",
    "# opzionale: salva per analisi locale\n",
    "np.save(\"pred_test_embeddings.npy\", pred_embds_test.numpy())\n",
    "\n",
    "# genera CSV (funzione del prof accetta anche torch.Tensor)\n",
    "submission = generate_submission(test_ids, pred_embds_test, 'submission.csv')\n",
    "print(f\"Model saved to: {MODEL_PATH}\")\n",
    "print(\"Submission saved to: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
