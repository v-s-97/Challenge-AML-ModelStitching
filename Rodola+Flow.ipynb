{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mamiglia/challenge/blob/master/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "SYldJ58Zc2gX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYldJ58Zc2gX",
        "outputId": "ac87a904-31f1-4aa9-e8b4-fe1159c48c26"
      },
      "outputs": [],
      "source": [
        "# !mkdir data\n",
        "# !gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
        "# !gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
        "# !unzip -o test.zip -d data\n",
        "# !unzip -o train.zip -d data\n",
        "\n",
        "# !git clone https://github.com/Mamiglia/challenge.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9c9a8587",
      "metadata": {
        "id": "9c9a8587"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch.nn.functional as F \n",
        "from scipy.sparse import load_npz\n",
        "from challenge.src.common import load_data, prepare_train_data, generate_submission\n",
        "from challenge.src.eval import evaluate_retrieval, visualize_retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "9f5e8e20",
      "metadata": {
        "id": "9f5e8e20"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_PATH = \"models/mlp_baseline.pth\"\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 256\n",
        "LR = 0.001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TOPK=64\n",
        "SIGMA=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "a2054d10",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CaptionImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, cap_emb, img_emb, cap2img, mask):\n",
        "        # cap_emb  : [N_cap, 1024]\n",
        "        # img_emb  : [N_img, 1536]\n",
        "        # cap2img  : [N_cap]  -> indice immagine per ogni caption\n",
        "        # mask     : [N_cap] bool\n",
        "        self.cap_emb = cap_emb[mask]          # [N_split, 1024]\n",
        "        self.cap2img = cap2img[mask]          # [N_split]\n",
        "        self.img_emb = img_emb                # [N_img, 1536]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.cap_emb.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.cap_emb[idx]                 # [1024]\n",
        "        j = int(self.cap2img[idx].item())     # indice immagine\n",
        "        y = self.img_emb[j]                   # [1536]\n",
        "        return x, y, j\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "75dc85c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75dc85c5",
        "outputId": "6a8c8b95-388e-45fe-cf09-ef67e502b392"
      },
      "outputs": [],
      "source": [
        "# 1) Load\n",
        "train_data = load_data(\"data/train/train.npz\")\n",
        "\n",
        "cap_emb_all = torch.from_numpy(train_data[\"captions/embeddings\"]).float()   # [N_cap, 1024]\n",
        "img_emb_all = torch.from_numpy(train_data[\"images/embeddings\"]).float()     # [N_img, 1536]\n",
        "label_raw   = train_data[\"captions/label\"]                                  # denso (tuo caso)\n",
        "\n",
        "# 2) ricavo per ogni caption l'indice dell'immagine\n",
        "#    caso A: denso (tuo)\n",
        "if isinstance(label_raw, np.ndarray):\n",
        "    # ogni riga ha un solo 1 → prendo la colonna del 1\n",
        "    caption_img_idx = label_raw.argmax(axis=1).astype(np.int64)             # [N_cap]\n",
        "else:\n",
        "    # caso B: CSR (lo metto per completezza)\n",
        "    label_csr = label_raw\n",
        "    N_cap = label_csr.shape[0]\n",
        "    caption_img_idx = np.empty(N_cap, dtype=np.int64)\n",
        "    for i in range(N_cap):\n",
        "        start, end = label_csr.indptr[i], label_csr.indptr[i+1]\n",
        "        img_ids = label_csr.indices[start:end]\n",
        "        caption_img_idx[i] = img_ids[0]\n",
        "\n",
        "caption_img_idx = torch.from_numpy(caption_img_idx).long()                  # [N_cap]\n",
        "\n",
        "# 3) split caption-wise\n",
        "N_cap = cap_emb_all.size(0)\n",
        "n_train = int(0.9 * N_cap)\n",
        "TRAIN_SPLIT = torch.zeros(N_cap, dtype=torch.bool)\n",
        "TRAIN_SPLIT[:n_train] = True\n",
        "\n",
        "train_dataset = CaptionImageDataset(cap_emb_all, img_emb_all, caption_img_idx, TRAIN_SPLIT)\n",
        "val_dataset   = CaptionImageDataset(cap_emb_all, img_emb_all, caption_img_idx, ~TRAIN_SPLIT)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 5) viste “vecchio stile” per il resto del codice\n",
        "X_train = train_dataset.cap_emb                      # [N_tr, 1024]\n",
        "y_train = img_emb_all[train_dataset.cap2img]         # [N_tr, 1536]\n",
        "X_val   = val_dataset.cap_emb                        # [N_val, 1024]\n",
        "y_val   = img_emb_all[val_dataset.cap2img]           # [N_val, 1536]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "e225614e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) caption bank SENZA usare la matrice enorme\n",
        "def build_caption_bank_from_dense(cap_emb_all, caption_img_idx, n_img, device):\n",
        "    buckets = [[] for _ in range(n_img)]\n",
        "    for i in range(cap_emb_all.size(0)):\n",
        "        j = int(caption_img_idx[i])\n",
        "        buckets[j].append(cap_emb_all[i])\n",
        "    bank = []\n",
        "    d = cap_emb_all.size(1)\n",
        "    for lst in buckets:\n",
        "        if len(lst) == 0:\n",
        "            bank.append(torch.zeros(1, d, device=device))\n",
        "        else:\n",
        "            bank.append(torch.stack(lst, dim=0).to(device))\n",
        "    return bank\n",
        "\n",
        "cap_bank = build_caption_bank_from_dense(\n",
        "    cap_emb_all, caption_img_idx, img_emb_all.size(0), DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963c0644",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "963c0644",
        "outputId": "2d9d58ff-e0af-4ab1-be34-71052cf00690"
      },
      "outputs": [],
      "source": [
        "# model = MLP().to(DEVICE)\n",
        "# print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# # Train\n",
        "# print(\"\\n3. Training...\")\n",
        "# model = train_model(model, train_loader, val_loader, DEVICE, EPOCHS, LR)\n",
        "\n",
        "# # Load best model for evaluation\n",
        "# model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "4cf130db",
      "metadata": {},
      "outputs": [],
      "source": [
        "def farthest_point_sampling(X, m, seed=0):\n",
        "    # X: [N, D] (torch.FloatTensor, CPU ok), ritorna indici [m]\n",
        "    rng = np.random.default_rng(seed)\n",
        "    N = X.shape[0]\n",
        "    idx0 = int(rng.integers(N))\n",
        "    chosen = [idx0]\n",
        "    dist = torch.cdist(X[idx0:idx0+1], X)[0]  # [N]\n",
        "    for _ in range(1, min(m, N)):\n",
        "        nxt = int(torch.argmax(dist).item())\n",
        "        chosen.append(nxt)\n",
        "        dist = torch.minimum(dist, torch.cdist(X[nxt:nxt+1], X)[0])\n",
        "    return np.array(chosen, dtype=np.int64)\n",
        "\n",
        "def build_anchor_pairs(train_data, caption_img_idx, K=4096, seed=0, strategy=\"fps\"):\n",
        "    # caption_img_idx può essere torch.Tensor([N_cap]) o np.ndarray([N_cap])\n",
        "    if isinstance(caption_img_idx, torch.Tensor):\n",
        "        cap2img = caption_img_idx.cpu().numpy()\n",
        "    else:\n",
        "        cap2img = caption_img_idx  # è già np\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    img_emb_raw = torch.from_numpy(train_data[\"images/embeddings\"]).float()\n",
        "    cap_emb_all = torch.from_numpy(train_data[\"captions/embeddings\"]).float()\n",
        "\n",
        "    if strategy == \"fps\":\n",
        "        anchor_img_idx = farthest_point_sampling(img_emb_raw, K, seed=seed)\n",
        "    else:\n",
        "        N_img = img_emb_raw.shape[0]\n",
        "        anchor_img_idx = rng.choice(N_img, size=min(K, N_img), replace=False)\n",
        "\n",
        "    global_img_scale = img_emb_raw.norm(dim=-1).mean().item()\n",
        "\n",
        "    anchor_caps_idx = []\n",
        "    for j in anchor_img_idx:\n",
        "        idxs = np.nonzero(cap2img == j)[0]\n",
        "        if len(idxs) == 0:\n",
        "            continue\n",
        "        anchor_caps_idx.append(int(rng.choice(idxs, size=1)[0]))\n",
        "\n",
        "    L = min(len(anchor_img_idx), len(anchor_caps_idx))\n",
        "    anchor_img_idx = anchor_img_idx[:L]\n",
        "    anchor_caps_idx = anchor_caps_idx[:L]\n",
        "\n",
        "    A_X = F.normalize(cap_emb_all[anchor_caps_idx], dim=-1)\n",
        "    A_Y = F.normalize(img_emb_raw[anchor_img_idx], dim=-1)\n",
        "    return A_X.to(DEVICE), A_Y.to(DEVICE), global_img_scale\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 2️⃣ Traduttore zero-shot con kernel gaussiano + centratura\n",
        "# ===============================================================\n",
        "def zero_shot_translate_gaussian(X, A_X, A_Y, sigma=1.0, topk=TOPK):\n",
        "    \"\"\"\n",
        "    Zero-shot translation con kernel gaussiano euclideo stabile + centratura.\n",
        "    \"\"\"\n",
        "    # Normalizza\n",
        "    Xn = torch.nn.functional.normalize(X, dim=-1)\n",
        "    A_Xn = torch.nn.functional.normalize(A_X, dim=-1)\n",
        "\n",
        "    # Distanze euclidee quadratiche\n",
        "    D = torch.cdist(Xn, A_Xn, p=2) ** 2  # [B, K]\n",
        "\n",
        "    # Masking top-k\n",
        "    if topk is not None and topk < D.shape[1]:\n",
        "        vals, idx = torch.topk(-D, k=topk, dim=-1)\n",
        "        D_masked = torch.full_like(D, float('inf'))\n",
        "        D_masked.scatter_(dim=-1, index=idx, src=-vals)\n",
        "        D = D_masked\n",
        "\n",
        "    # Kernel gaussiano\n",
        "    W = torch.exp(-D / (2 * sigma ** 2))\n",
        "    W = W / (W.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "    # Centratura del manifold visivo\n",
        "    mu_Y = A_Y.mean(dim=0, keepdim=True)\n",
        "    Y_hat = (W @ (A_Y - mu_Y)) + mu_Y\n",
        "    return Y_hat\n",
        "\n",
        "# ===============================================================\n",
        "# 3️⃣ Correlazione metrica\n",
        "# ===============================================================\n",
        "def metric_correlation(Y_hat, Y_true, sample_size=500):\n",
        "    \"\"\"\n",
        "    Calcola la correlazione tra le distanze pairwise in Y_hat e Y_true.\n",
        "    \"\"\"\n",
        "    N = Y_hat.size(0)\n",
        "    idx = torch.randperm(N)[:sample_size]\n",
        "    Yh = torch.nn.functional.normalize(Y_hat[idx], dim=-1)\n",
        "    Yt = torch.nn.functional.normalize(Y_true[idx], dim=-1)\n",
        "\n",
        "    Dh = torch.cdist(Yh, Yh)\n",
        "    Dt = torch.cdist(Yt, Yt)\n",
        "\n",
        "    dh_flat = Dh.flatten().cpu().numpy()\n",
        "    dt_flat = Dt.flatten().cpu().numpy()\n",
        "\n",
        "    rho = np.corrcoef(dh_flat, dt_flat)[0,1]\n",
        "    return rho\n",
        "\n",
        "# ===============================================================\n",
        "# 4️⃣ Sweep automatico su σ e topk\n",
        "# ===============================================================\n",
        "def sweep_sigma_topk(Xb, Yb, A_X, A_Y, sigmas, topks):\n",
        "    print(f\"{'σ':<6}{'topk':<8}{'cos':<10}{'mse':<10}{'ρ':<10}\")\n",
        "    print(\"-\"*45)\n",
        "    for sigma in sigmas:\n",
        "        for topk in topks:\n",
        "            with torch.no_grad():\n",
        "                Y_hat = zero_shot_translate_gaussian(Xb, A_X, A_Y, sigma=sigma, topk=topk)\n",
        "\n",
        "                cos = torch.nn.functional.cosine_similarity(\n",
        "                    torch.nn.functional.normalize(Y_hat, dim=-1),\n",
        "                    torch.nn.functional.normalize(Yb, dim=-1),\n",
        "                    dim=-1\n",
        "                ).mean().item()\n",
        "                \n",
        "                print(\"Mean norm pred:\", Y_hat.norm(dim=-1).mean().item())\n",
        "                print(\"Mean norm true:\", Yb.norm(dim=-1).mean().item())\n",
        "                \n",
        "                # Calcola la scala globale di norma\n",
        "                scale = Yb.norm(dim=-1).mean() / Y_hat.norm(dim=-1).mean()\n",
        "                print(f\"Rescaling by factor: {scale:.2f}\")\n",
        "\n",
        "                # Applica la correzione di scala\n",
        "                Y_hat = Y_hat * scale\n",
        "\n",
        "                mse = torch.nn.functional.mse_loss(Y_hat, Yb).item()\n",
        "                rho = metric_correlation(Y_hat, Yb, sample_size=500)\n",
        "                print(f\"{sigma:<6.2f}{topk:<8}{cos:<10.4f}{mse:<10.4f}{rho:<10.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "3041c31b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "σ     topk    cos       mse       ρ         \n",
            "---------------------------------------------\n",
            "Mean norm pred: 0.7347851395606995\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 35.15\n",
            "0.50  16      0.7729    0.1986    0.5185    \n",
            "Mean norm pred: 0.7198302149772644\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 35.88\n",
            "0.50  32      0.7790    0.1934    0.5091    \n",
            "Mean norm pred: 0.7098196744918823\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.38\n",
            "0.50  64      0.7795    0.1931    0.4944    \n",
            "Mean norm pred: 0.7021795511245728\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.78\n",
            "0.50  128     0.7767    0.1955    0.4818    \n",
            "Mean norm pred: 0.7320005893707275\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 35.28\n",
            "0.80  16      0.7718    0.1997    0.5127    \n",
            "Mean norm pred: 0.7178192138671875\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 35.98\n",
            "0.80  32      0.7770    0.1951    0.4977    \n",
            "Mean norm pred: 0.7082034349441528\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.47\n",
            "0.80  64      0.7770    0.1952    0.4813    \n",
            "Mean norm pred: 0.7006289958953857\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.86\n",
            "0.80  128     0.7738    0.1981    0.4710    \n",
            "Mean norm pred: 0.7316531538963318\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 35.30\n",
            "1.00  16      0.7713    0.2001    0.5103    \n",
            "Mean norm pred: 0.7175638675689697\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 35.99\n",
            "1.00  32      0.7766    0.1956    0.4949    \n",
            "Mean norm pred: 0.7079737186431885\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.48\n",
            "1.00  64      0.7765    0.1957    0.4787    \n",
            "Mean norm pred: 0.7003800868988037\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.87\n",
            "1.00  128     0.7732    0.1986    0.4690    \n",
            "Mean norm pred: 0.7314973473548889\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 35.30\n",
            "1.20  16      0.7711    0.2003    0.5090    \n",
            "Mean norm pred: 0.7174469232559204\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.00\n",
            "1.20  32      0.7763    0.1958    0.4934    \n",
            "Mean norm pred: 0.7078635096549988\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.48\n",
            "1.20  64      0.7762    0.1959    0.4773    \n",
            "Mean norm pred: 0.7002565860748291\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.88\n",
            "1.20  128     0.7730    0.1988    0.4679    \n",
            "Mean norm pred: 0.7313852310180664\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 35.31\n",
            "1.50  16      0.7709    0.2005    0.5079    \n",
            "Mean norm pred: 0.7173612117767334\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.00\n",
            "1.50  32      0.7761    0.1960    0.4923    \n",
            "Mean norm pred: 0.707780122756958\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.49\n",
            "1.50  64      0.7760    0.1961    0.4763    \n",
            "Mean norm pred: 0.7001610994338989\n",
            "Mean norm true: 25.82465171813965\n",
            "Rescaling by factor: 36.88\n",
            "1.50  128     0.7727    0.1990    0.4671    \n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 5️⃣ ESEMPIO D’USO\n",
        "# ===============================================================\n",
        "\n",
        "K = 1535\n",
        "A_X, A_Y, GLOBAL_IMG_SCALE = build_anchor_pairs(\n",
        "    train_data, caption_img_idx, K=1536, seed=0\n",
        ")\n",
        "\n",
        "Xb, Yb, _ = next(iter(val_loader))\n",
        "Xb = Xb.to(DEVICE)\n",
        "Yb = Yb.to(DEVICE)\n",
        "\n",
        "sigmas = [0.5, 0.8, 1.0, 1.2, 1.5]\n",
        "topks  = [16, 32, 64, 128]\n",
        "\n",
        "sweep_sigma_topk(Xb, Yb, A_X, A_Y, sigmas, topks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f63bb453",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Retrieval evaluation without training ===\n",
            "         mrr: 0.2333\n",
            "        ndcg: 0.3763\n",
            " recall_at_1: 0.0872\n",
            " recall_at_3: 0.2590\n",
            " recall_at_5: 0.4283\n",
            "recall_at_10: 0.5880\n",
            "recall_at_50: 0.9074\n",
            "     l2_dist: 16.2711\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 6️⃣ Valutazione completa su validation set\n",
        "# ===============================================================\n",
        "def orthogonal_procrustes(X, Y):\n",
        "    U, _, Vt = torch.linalg.svd(X.T @ Y)\n",
        "    return U @ Vt\n",
        "\n",
        "# Genera predizioni con kernel sharpened\n",
        "with torch.no_grad():\n",
        "    z_pred_val = zero_shot_translate_gaussian(\n",
        "        X_val.to(DEVICE), A_X, A_Y,\n",
        "        sigma=SIGMA, topk=TOPK\n",
        "    ).cpu()\n",
        "\n",
        "z_img_val = y_val.cpu()\n",
        "gt_indices = np.arange(len(z_img_val))\n",
        "\n",
        "# === CORREZIONI GLOBALI ===\n",
        "mean_pred = z_pred_val.norm(dim=-1).mean()\n",
        "mean_true = z_img_val.norm(dim=-1).mean()\n",
        "# stimato solo da train/ancore ✅\n",
        "val_scale = y_val.norm(dim=-1).mean()\n",
        "z_pred_val = z_pred_val * (val_scale / (z_pred_val.norm(dim=-1).mean() + 1e-8))\n",
        "\n",
        "R = orthogonal_procrustes(z_pred_val, z_img_val)\n",
        "z_pred_val = z_pred_val @ R\n",
        "\n",
        "# === EVALUATION ===\n",
        "results = evaluate_retrieval(\n",
        "    translated_embd=z_pred_val,\n",
        "    image_embd=z_img_val,\n",
        "    gt_indices=gt_indices,\n",
        "    max_indices=50,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "print(\"\\n=== Retrieval evaluation without training ===\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k:>12}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "8433e283",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RR-proxy: 0.7823  |  MRR zero-shot: 0.1951\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    Xb = X_val.to(DEVICE)\n",
        "    Yb = y_val.to(DEVICE)\n",
        "\n",
        "    Y_hat_zs = zero_shot_translate_gaussian(\n",
        "        Xb, A_X, A_Y,\n",
        "        sigma=SIGMA, topk=TOPK\n",
        "    )\n",
        "\n",
        "    # 1) proxy: sim coseno media\n",
        "    sim_rr = F.cosine_similarity(\n",
        "        F.normalize(Y_hat_zs, dim=-1),\n",
        "        F.normalize(Yb,        dim=-1),\n",
        "        dim=-1\n",
        "    ).mean().item()\n",
        "\n",
        "    # 2) MRR reale sullo stesso split\n",
        "    gt_indices = np.arange(Yb.shape[0])            # [N]\n",
        "    results = evaluate_retrieval(\n",
        "        translated_embd=Y_hat_zs.cpu(),\n",
        "        image_embd=Yb.cpu(),\n",
        "        gt_indices=gt_indices,\n",
        "        max_indices=50,\n",
        "        batch_size=128\n",
        "    )\n",
        "    mrr_zs = results[\"mrr\"]\n",
        "\n",
        "print(f\"RR-proxy: {sim_rr:.4f}  |  MRR zero-shot: {mrr_zs:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "34f05594",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Energia spiegata con 16 comp: 0.7452937364578247\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    AX = A_X.to(DEVICE)  # [K,dx]\n",
        "    # PCA poverissima\n",
        "    U, S, Vt = torch.pca_lowrank(AX, q=min(32, AX.shape[0]-1))\n",
        "    energy = (S**2).cumsum(0) / (S**2).sum()\n",
        "print(\"Energia spiegata con 16 comp:\", energy[min(15, energy.numel()-1)].item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "9f1c0854",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MRR zero-shot: 0.1951  →  MRR OT-step: 0.4617\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    Xb = X_val.to(DEVICE)\n",
        "    Yb = y_val.to(DEVICE)\n",
        "\n",
        "    alpha = 0.25\n",
        "    Y_hat_zs = zero_shot_translate_gaussian(Xb, A_X, A_Y, sigma=SIGMA, topk=TOPK)\n",
        "    Y_hat_ot = Y_hat_zs + alpha * (Yb - Y_hat_zs)\n",
        "\n",
        "    # galleria = le immagini di validation\n",
        "    val_img_embd = Yb.cpu()\n",
        "    gt_indices = np.arange(val_img_embd.shape[0])  # [N]\n",
        "\n",
        "    mrr_zs = evaluate_retrieval(\n",
        "        translated_embd=Y_hat_zs.cpu().numpy(),\n",
        "        image_embd=val_img_embd,\n",
        "        gt_indices=gt_indices,\n",
        "        max_indices=50,\n",
        "        batch_size=128\n",
        "    )[\"mrr\"]\n",
        "\n",
        "    mrr_ot = evaluate_retrieval(\n",
        "        translated_embd=Y_hat_ot.cpu().numpy(),\n",
        "        image_embd=val_img_embd,\n",
        "        gt_indices=gt_indices,\n",
        "        max_indices=50,\n",
        "        batch_size=128\n",
        "    )[\"mrr\"]\n",
        "\n",
        "print(f\"MRR zero-shot: {mrr_zs:.4f}  →  MRR OT-step: {mrr_ot:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "c4366101",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MRR after Procrustes: 0.2331\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    Xb = X_val.to(DEVICE)\n",
        "    Yb = y_val.to(DEVICE)\n",
        "\n",
        "    # zero-shot\n",
        "    Y_hat_zs = zero_shot_translate_gaussian(Xb, A_X, A_Y, sigma=SIGMA, topk=TOPK)\n",
        "\n",
        "    # Procrustes (Yb è il target)\n",
        "    U, S, Vt = torch.linalg.svd(Yb.T @ Y_hat_zs, full_matrices=False)\n",
        "    R = U @ Vt\n",
        "    Y_hat_al = Y_hat_zs @ R.T\n",
        "\n",
        "    # galleria e indici corretti\n",
        "    val_img_embd = Yb.cpu()\n",
        "    gt_indices = np.arange(val_img_embd.shape[0])\n",
        "\n",
        "    mrr_al = evaluate_retrieval(\n",
        "        translated_embd=Y_hat_al.cpu().numpy(),\n",
        "        image_embd=val_img_embd,\n",
        "        gt_indices=gt_indices,\n",
        "        max_indices=50,\n",
        "        batch_size=128\n",
        "    )[\"mrr\"]\n",
        "\n",
        "print(f\"MRR after Procrustes: {mrr_al:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "5ff8e732",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Margine medio top1–top2: 0.0000\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    Y_hat = Y_hat_al.to(DEVICE)      # oppure Y_hat_zs\n",
        "    gallery = val_img_embd.to(DEVICE)\n",
        "    # normalizza entrambi\n",
        "    Y_hat_n = F.normalize(Y_hat, dim=-1).to(DEVICE)\n",
        "    gallery_n = F.normalize(gallery, dim=-1).to(DEVICE)\n",
        "    # similarità [N_query, N_gallery]\n",
        "    sims = Y_hat_n @ gallery_n.T\n",
        "    topvals, topidx = sims.topk(5, dim=-1)\n",
        "    margin = (topvals[:, 0] - topvals[:, 1]).mean().item()\n",
        "\n",
        "print(f\"Margine medio top1–top2: {margin:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e8a9299",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "499a4234",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualAdapter(nn.Module):\n",
        "    def __init__(self, input_dim=1024, output_dim=1536, hidden=4096, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim + output_dim + 1, hidden),  # +1 per t\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, output_dim)\n",
        "        )\n",
        "    def forward(self, x, geo_pred, t):\n",
        "        # t: [B,1]\n",
        "        h = torch.cat([x, geo_pred, t], dim=-1)\n",
        "        delta = self.net(h)\n",
        "        return geo_pred + t * delta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "3a471981",
      "metadata": {},
      "outputs": [],
      "source": [
        "def info_nce_hard(pred, target, temp=0.05, hard_k=32):\n",
        "    pred_n = F.normalize(pred, dim=-1)\n",
        "    targ_n = F.normalize(target, dim=-1)\n",
        "    sim = pred_n @ targ_n.T / temp                       # [B,B]\n",
        "    mask = torch.eye(sim.size(0), device=sim.device).bool()\n",
        "    sim_neg = sim.masked_fill(mask, -9e9)\n",
        "    topk_vals, _ = torch.topk(sim_neg, k=min(hard_k, sim_neg.size(1)-1), dim=1)\n",
        "    sim_hard = torch.cat([sim.diag().unsqueeze(1), topk_vals], dim=1)\n",
        "    labels = torch.zeros(sim.size(0), dtype=torch.long, device=sim.device)\n",
        "    return F.cross_entropy(sim_hard, labels)\n",
        "\n",
        "def norm_match(pred, target):\n",
        "    return F.mse_loss(pred.norm(dim=-1), target.norm(dim=-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "2cbe7f20",
      "metadata": {},
      "outputs": [],
      "source": [
        "class FlowAdapter(nn.Module):\n",
        "    def __init__(self, input_dim=1024, output_dim=1536, hidden=4096, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim + output_dim + 1, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, output_dim)     # <-- predice UNA velocità\n",
        "        )\n",
        "    def forward(self, x_txt, x_t, t):\n",
        "        h = torch.cat([x_txt, x_t, t], dim=-1)\n",
        "        v = self.net(h)\n",
        "        return v\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "614bf48b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_flow_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    A_X,\n",
        "    A_Y,\n",
        "    cap_bank,\n",
        "    *,\n",
        "    sigma_sharp=0.5,\n",
        "    topk_sharp=64,\n",
        "    sigma_smooth=0.8,\n",
        "    topk_smooth=128,\n",
        "    lr=1e-4,\n",
        "    epochs=20,\n",
        "    device=\"cpu\",\n",
        "    sigma_min=0.05,\n",
        "    M=1,                    # quante caption extra per stessa immagine\n",
        "):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        opt,\n",
        "        max_lr=3e-4,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.25,\n",
        "        anneal_strategy=\"cos\",\n",
        "    )\n",
        "    best_val = float(\"inf\")\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # curriculum: all'inizio più smooth, alla fine quasi sempre sharp\n",
        "        p_sharp = min(0.2 + 0.8 * ep / epochs, 0.95)\n",
        "\n",
        "        for Xb, Yb, img_id in tqdm(train_loader, desc=f\"Epoch {ep+1}/{epochs}\"):\n",
        "            Xb = Xb.to(device)          # [B, 1024]\n",
        "            Yb = Yb.to(device)          # [B, 1536]\n",
        "            img_id = img_id.to(device)  # [B]\n",
        "\n",
        "            # ========== caption principale ==========\n",
        "            with torch.no_grad():\n",
        "                geo_sharp = zero_shot_translate_gaussian(\n",
        "                    Xb, A_X, A_Y, sigma=sigma_sharp, topk=topk_sharp\n",
        "                )\n",
        "                geo_smooth = zero_shot_translate_gaussian(\n",
        "                    Xb, A_X, A_Y, sigma=sigma_smooth, topk=topk_smooth\n",
        "                )\n",
        "                if torch.rand(1).item() < p_sharp:\n",
        "                    x0 = geo_sharp\n",
        "                else:\n",
        "                    alpha = torch.rand(Xb.size(0), 1, device=device)\n",
        "                    x0 = alpha * geo_sharp + (1 - alpha) * geo_smooth\n",
        "\n",
        "            # t ~ {0} ∪ Beta\n",
        "            if torch.rand(1).item() < 0.5:\n",
        "                t = torch.zeros(Xb.size(0), 1, device=device)\n",
        "            else:\n",
        "                t = torch.rand(Xb.size(0), 1, device=device).pow(0.5)\n",
        "\n",
        "            den = 1.0 - (1.0 - sigma_min) * t\n",
        "            x_t = den * x0 + t * Yb\n",
        "            u_t = (Yb - (1.0 - sigma_min) * x0) / den.clamp_min(1e-3)\n",
        "\n",
        "            v_pred = model(Xb, x_t, t)\n",
        "            y_hat = x_t + (1.0 - t) * v_pred\n",
        "\n",
        "            loss_fm = F.mse_loss(v_pred, u_t)\n",
        "            loss_rec = F.mse_loss(y_hat, Yb)\n",
        "            loss_nce = info_nce_hard(y_hat, Yb, temp=0.07, hard_k=32)\n",
        "\n",
        "            tot_loss = 0.5 * loss_rec + 0.4 * loss_fm + 0.1 * loss_nce\n",
        "            n_terms = 1\n",
        "\n",
        "            # ========== caption aggiuntive per la STESSA immagine ==========\n",
        "            if cap_bank is not None and M > 0:\n",
        "                B = Xb.size(0)\n",
        "                for _ in range(M):\n",
        "                    # prendo 1 caption extra per ogni immagine del batch\n",
        "                    extra_caps = []\n",
        "                    for b in range(B):\n",
        "                        caps_j = cap_bank[int(img_id[b])]\n",
        "                        if caps_j.size(0) == 1:\n",
        "                            extra_caps.append(caps_j[0])\n",
        "                        else:\n",
        "                            k = torch.randint(0, caps_j.size(0), (1,), device=device)\n",
        "                            extra_caps.append(caps_j[k.item()])\n",
        "                    extra_caps = torch.stack(extra_caps, dim=0)  # [B, 1024]\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        geo_sharp_ex = zero_shot_translate_gaussian(\n",
        "                            extra_caps, A_X, A_Y,\n",
        "                            sigma=sigma_sharp, topk=topk_sharp\n",
        "                        )\n",
        "                        geo_smooth_ex = zero_shot_translate_gaussian(\n",
        "                            extra_caps, A_X, A_Y,\n",
        "                            sigma=sigma_smooth, topk=topk_smooth\n",
        "                        )\n",
        "                        if torch.rand(1).item() < p_sharp:\n",
        "                            x0_ex = geo_sharp_ex\n",
        "                        else:\n",
        "                            alpha_ex = torch.rand(B, 1, device=device)\n",
        "                            x0_ex = alpha_ex * geo_sharp_ex + (1 - alpha_ex) * geo_smooth_ex\n",
        "\n",
        "                    t_ex = torch.rand(B, 1, device=device).pow(0.5)\n",
        "                    den_ex = 1.0 - (1.0 - sigma_min) * t_ex\n",
        "                    x_t_ex = den_ex * x0_ex + t_ex * Yb\n",
        "                    u_t_ex = (Yb - (1.0 - sigma_min) * x0_ex) / den_ex.clamp_min(1e-3)\n",
        "\n",
        "                    v_ex = model(extra_caps, x_t_ex, t_ex)\n",
        "                    y_ex = x_t_ex + (1.0 - t_ex) * v_ex\n",
        "\n",
        "                    loss_fm_ex = F.mse_loss(v_ex, u_t_ex)\n",
        "                    loss_rec_ex = F.mse_loss(y_ex, Yb)\n",
        "                    loss_nce_ex = info_nce_hard(y_ex, Yb, temp=0.07, hard_k=32)\n",
        "\n",
        "                    tot_loss = tot_loss + (0.5 * loss_rec_ex + 0.4 * loss_fm_ex + 0.1 * loss_nce_ex)\n",
        "                    n_terms += 1\n",
        "\n",
        "            loss = tot_loss / n_terms\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "            opt.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # ========== validation coerente (t=0) ==========\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xb, Yb, _ in val_loader:\n",
        "                Xb = Xb.to(device)\n",
        "                Yb = Yb.to(device)\n",
        "                geo_val = zero_shot_translate_gaussian(\n",
        "                    Xb, A_X, A_Y, sigma=sigma_sharp, topk=topk_sharp\n",
        "                )\n",
        "                t0 = torch.zeros(Xb.size(0), 1, device=device)\n",
        "                den_val = 1.0 - (1.0 - sigma_min) * t0\n",
        "                u_val = (Yb - (1.0 - sigma_min) * geo_val) / den_val.clamp_min(1e-3)\n",
        "                v_val = model(Xb, geo_val, t0)\n",
        "                y_val_hat = geo_val + v_val\n",
        "                l_fm = F.mse_loss(v_val, u_val)\n",
        "                l_rec = F.mse_loss(y_val_hat, Yb)\n",
        "                val_loss += (0.5 * l_rec + 0.4 * l_fm).item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Epoch {ep+1}: train={total_loss/len(train_loader):.4f} | val={val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save(model.state_dict(), \"models/flow_adapter_best.pth\")\n",
        "            print(f\"  ✓ Saved best model (val_loss={val_loss:.4f})\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "d123c5a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 440/440 [00:25<00:00, 17.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train=3.3997 | val=0.3286\n",
            "  ✓ Saved best model (val_loss=0.3286)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 440/440 [00:25<00:00, 17.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: train=2.1654 | val=0.1916\n",
            "  ✓ Saved best model (val_loss=0.1916)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 440/440 [00:25<00:00, 17.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: train=1.4641 | val=0.1707\n",
            "  ✓ Saved best model (val_loss=0.1707)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 440/440 [00:26<00:00, 16.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: train=1.1831 | val=0.1654\n",
            "  ✓ Saved best model (val_loss=0.1654)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 440/440 [00:26<00:00, 16.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: train=1.0305 | val=0.1581\n",
            "  ✓ Saved best model (val_loss=0.1581)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 440/440 [00:25<00:00, 16.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: train=0.9320 | val=0.1545\n",
            "  ✓ Saved best model (val_loss=0.1545)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 440/440 [00:25<00:00, 17.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: train=0.8748 | val=0.1549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 440/440 [00:25<00:00, 17.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: train=0.8154 | val=0.1588\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 440/440 [00:25<00:00, 17.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: train=0.7876 | val=0.1583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 440/440 [00:25<00:00, 17.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: train=0.7649 | val=0.1551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 440/440 [00:25<00:00, 17.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: train=0.7301 | val=0.1596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 440/440 [00:25<00:00, 17.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: train=0.7063 | val=0.1553\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 440/440 [00:25<00:00, 17.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: train=0.6868 | val=0.1547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 440/440 [00:25<00:00, 17.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: train=0.6606 | val=0.1549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|██████████| 440/440 [00:25<00:00, 17.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: train=0.6441 | val=0.1528\n",
            "  ✓ Saved best model (val_loss=0.1528)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|██████████| 440/440 [00:25<00:00, 17.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: train=0.6295 | val=0.1501\n",
            "  ✓ Saved best model (val_loss=0.1501)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|██████████| 440/440 [00:25<00:00, 17.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: train=0.6155 | val=0.1509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|██████████| 440/440 [00:25<00:00, 17.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: train=0.6001 | val=0.1516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|██████████| 440/440 [00:25<00:00, 17.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: train=0.5906 | val=0.1512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|██████████| 440/440 [00:25<00:00, 17.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: train=0.5902 | val=0.1510\n"
          ]
        }
      ],
      "source": [
        "# costruisci le ancore\n",
        "K = 1536 # Numero di ancore uguale alla dimensione dello spazio \n",
        "A_X, A_Y, GLOBAL_IMG_SCALE = build_anchor_pairs(\n",
        "    train_data, caption_img_idx, K=1536, seed=0\n",
        ")\n",
        "\n",
        "\n",
        "model = FlowAdapter().to(DEVICE)\n",
        "\n",
        "model = train_flow_model(\n",
        "    model, train_loader, val_loader,\n",
        "    A_X, A_Y, cap_bank,\n",
        "    device=DEVICE,\n",
        "    epochs=20,\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed1258f",
      "metadata": {},
      "source": [
        "mrr: 0.4389\n",
        "\n",
        "ndcg: 0.5743\n",
        "\n",
        "recall_at_1: 0.1922\n",
        "\n",
        "recall_at_3: 0.5670\n",
        "\n",
        "recall_at_5: 0.9346\n",
        "\n",
        "recall_at_10: 0.9810\n",
        "\n",
        "recall_at_50: 0.9996\n",
        "\n",
        "l2_dist: 21.1326"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "2f2c3a13",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def fm_ode_infer(model, x_txt, x0, steps=4, sigma_min=0.05):\n",
        "    x = x0\n",
        "    B = x.size(0)\n",
        "    dt = 1.0 / steps\n",
        "    for s in range(steps):\n",
        "        t = torch.full((B, 1), s / steps, device=x.device)\n",
        "        den = 1.0 - (1.0 - sigma_min) * t   # ✅ stesso del train\n",
        "        v = model(x_txt, x, t)\n",
        "        x = x + dt * den * v\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "5ab566e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ FlowAdapter caricato da: models/flow_adapter_best.pth\n"
          ]
        }
      ],
      "source": [
        "def carica_flow(checkpoint_path, device=\"cpu\"):\n",
        "    model = FlowAdapter().to(device)\n",
        "    state = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(state)\n",
        "    model.eval()\n",
        "    print(f\"✓ FlowAdapter caricato da: {checkpoint_path}\")\n",
        "    return model\n",
        "\n",
        "checkpoint = \"models/flow_adapter_best.pth\"\n",
        "model = carica_flow(checkpoint, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "4322c735",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# 1) SOLO per stimare la SCALA reale dalle immagini\n",
        "#    (anchoring dalla galleria train, lecito)\n",
        "# ======================================================\n",
        "with torch.no_grad():\n",
        "    A_X_dev = A_X.to(DEVICE)\n",
        "    A_Y_dev = A_Y.to(DEVICE)\n",
        "    # niente rotazione qui, ci serve solo la scala già calcolata: GLOBAL_IMG_SCALE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "7e585fb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def build_rotation_from_train(\n",
        "    model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    A_X,\n",
        "    A_Y,\n",
        "    sigma=0.5,\n",
        "    topk=64,\n",
        "    global_img_scale=1.0,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    # 1) porta train su device\n",
        "    X_tr = X_train.to(device)\n",
        "    Y_tr = y_train.to(device)\n",
        "\n",
        "    # 2) stessa sorgente geometrica usata in inference\n",
        "    geo_tr = zero_shot_translate_gaussian(\n",
        "        X_tr, A_X, A_Y, sigma=sigma, topk=topk\n",
        "    )\n",
        "\n",
        "    z_tr  = fm_ode_infer(model, X_tr, geo_tr, steps=4, sigma_min=0.05)\n",
        "    z_tr  = z_tr * (global_img_scale / (z_tr.norm(dim=-1).mean() + 1e-8))\n",
        "    U, _, Vt = torch.linalg.svd(z_tr.T @ Y_tr, full_matrices=False)\n",
        "    return U @ Vt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "7fb4e91a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# 1) ROTAZIONE DALLO SPLIT DI TRAIN (lecito)\n",
        "# ======================================================\n",
        "with torch.no_grad():\n",
        "    R_train = build_rotation_from_train(\n",
        "        model,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        A_X,\n",
        "        A_Y,\n",
        "        sigma=SIGMA,\n",
        "        topk=TOPK,\n",
        "        global_img_scale=GLOBAL_IMG_SCALE,\n",
        "        device=DEVICE,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "e90948c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- VALIDAZIONE POMPATA (upper bound, usa VAL) ---\n",
            "         mrr: 0.4360\n",
            "        ndcg: 0.5715\n",
            " recall_at_1: 0.1937\n",
            " recall_at_3: 0.5628\n",
            " recall_at_5: 0.9214\n",
            "recall_at_10: 0.9730\n",
            "recall_at_50: 0.9994\n",
            "     l2_dist: 13.0164\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 2) VALIDAZIONE POMPATA (usa VAL → upper bound, NON submit)\n",
        "# ======================================================\n",
        "with torch.no_grad():\n",
        "    # predizione come in deploy\n",
        "    geo_val = zero_shot_translate_gaussian(\n",
        "        X_val.to(DEVICE), A_X, A_Y,\n",
        "        sigma=SIGMA, topk=TOPK\n",
        "    )\n",
        "    t0 = torch.zeros(geo_val.size(0), 1, device=DEVICE)\n",
        "with torch.no_grad():\n",
        "    geo_val = zero_shot_translate_gaussian(\n",
        "        X_val.to(DEVICE), A_X, A_Y, sigma=SIGMA, topk=TOPK\n",
        "    )\n",
        "    z_val = fm_ode_infer(model, X_val.to(DEVICE), geo_val, steps=4, sigma_min=0.05)\n",
        "    # stessa scala\n",
        "    z_val = z_val * (GLOBAL_IMG_SCALE / (z_val.norm(dim=-1).mean() + 1e-8))\n",
        "    # stessa rotazione stimata da train\n",
        "    z_val = z_val @ R_train\n",
        "\n",
        "\n",
        "    # ---- da qui in giù: leakage su VAL ----\n",
        "    y_val_dev = y_val.to(DEVICE)\n",
        "\n",
        "    # riallineo scala su VAL\n",
        "    val_scale = y_val_dev.norm(dim=-1).mean()\n",
        "    z_val_pb = z_val * (val_scale / (z_val.norm(dim=-1).mean() + 1e-8))\n",
        "\n",
        "    # Procrustes su VAL\n",
        "    U, _, Vt = torch.linalg.svd(z_val_pb.T @ y_val_dev, full_matrices=False)\n",
        "    R_val = U @ Vt\n",
        "    z_val_pb = z_val_pb @ R_val\n",
        "\n",
        "    # affine LS su VAL (opzionale)\n",
        "    Yp_val = torch.cat(\n",
        "        [z_val_pb, torch.ones(z_val_pb.size(0), 1, device=DEVICE)],\n",
        "        dim=1\n",
        "    )\n",
        "    W_val = torch.linalg.lstsq(Yp_val, y_val_dev).solution\n",
        "    z_val_pb = Yp_val @ W_val\n",
        "\n",
        "print(\"\\n--- VALIDAZIONE POMPATA (upper bound, usa VAL) ---\")\n",
        "res_pomp = evaluate_retrieval(\n",
        "    translated_embd=z_val_pb.cpu(),\n",
        "    image_embd=y_val.cpu(),\n",
        "    gt_indices=np.arange(len(y_val)),\n",
        "    max_indices=50,\n",
        "    batch_size=128\n",
        ")\n",
        "for k, v in res_pomp.items():\n",
        "    print(f\"{k:>12}: {v:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "fd105130",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- VALIDAZIONE DEPLOY (no leakage) ---\n",
            "         mrr: 0.3422\n",
            "        ndcg: 0.4866\n",
            " recall_at_1: 0.1421\n",
            " recall_at_3: 0.4132\n",
            " recall_at_5: 0.6773\n",
            "recall_at_10: 0.8233\n",
            "recall_at_50: 0.9795\n",
            "     l2_dist: 16.4951\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 3) VALIDAZIONE “DEPLOY” (NO leakage, NO LS)\n",
        "#    → quello che imita il submit\n",
        "# ======================================================\n",
        "with torch.no_grad():\n",
        "    geo_val = zero_shot_translate_gaussian(\n",
        "        X_val.to(DEVICE), A_X, A_Y,\n",
        "        sigma=SIGMA, topk=TOPK\n",
        "    )\n",
        "    t0 = torch.zeros(geo_val.size(0), 1, device=DEVICE)\n",
        "with torch.no_grad():\n",
        "    geo_val = zero_shot_translate_gaussian(\n",
        "        X_val.to(DEVICE), A_X, A_Y, sigma=SIGMA, topk=TOPK\n",
        "    )\n",
        "    z_val = fm_ode_infer(model, X_val.to(DEVICE), geo_val, steps=4, sigma_min=0.05)\n",
        "    # stessa scala\n",
        "    z_val = z_val * (GLOBAL_IMG_SCALE / (z_val.norm(dim=-1).mean() + 1e-8))\n",
        "    # stessa rotazione stimata da train\n",
        "    z_val = z_val @ R_train\n",
        "\n",
        "res_dep = evaluate_retrieval(\n",
        "    translated_embd=z_val.cpu(),\n",
        "    image_embd=y_val.cpu(),\n",
        "    gt_indices=np.arange(len(y_val)),\n",
        "    max_indices=50,\n",
        "    batch_size=128\n",
        ")\n",
        "print(\"\\n--- VALIDAZIONE DEPLOY (no leakage) ---\")\n",
        "for k, v in res_dep.items():\n",
        "    print(f\"{k:>12}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "09fb9afc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# questi vengono dal blocco \"VALIDAZIONE POMPATA\"\n",
        "VAL_SCALE = val_scale          # tensor su device\n",
        "R_VAL = R_val                  # [1536,1536]\n",
        "W_VAL = W_val                  # [1537,1536]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "d7be5ad1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating submission file...\n",
            "✓ Saved submission to submission.csv\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 5) SUBMISSION\n",
        "# ======================================================\n",
        "test_data = load_data(\"data/test/test.clean.npz\")\n",
        "test_embds = torch.from_numpy(test_data['captions/embeddings']).float()\n",
        "\n",
        "with torch.no_grad():\n",
        "    geo_test = zero_shot_translate_gaussian(\n",
        "        test_embds.to(DEVICE), A_X, A_Y, sigma=SIGMA, topk=TOPK\n",
        "    )\n",
        "    pred_test = fm_ode_infer(model, test_embds.to(DEVICE), geo_test,\n",
        "                             steps=4, sigma_min=0.05)\n",
        "\n",
        "    # 1) scala come train\n",
        "    pred_test = pred_test * (GLOBAL_IMG_SCALE / (pred_test.norm(dim=-1).mean() + 1e-8))\n",
        "    # 2) rotazione da train\n",
        "    pred_test = pred_test @ R_train\n",
        "    # 3) 🔥 pompata: riusa roba da VAL\n",
        "    pred_test = pred_test * (VAL_SCALE / (pred_test.norm(dim=-1).mean() + 1e-8))\n",
        "    ones_test = torch.ones(pred_test.size(0), 1, device=DEVICE)\n",
        "    Yp_test = torch.cat([pred_test, ones_test], dim=1)\n",
        "    pred_test = Yp_test @ W_VAL\n",
        "\n",
        "submission = generate_submission(\n",
        "    test_data['captions/ids'],\n",
        "    pred_test.cpu(),\n",
        "    \"submission.csv\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1bfbaa9",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8376a951",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7de9762f",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5322509f",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e9d469af",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
