{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d8860f",
   "metadata": {},
   "source": [
    "# Challenge Advanced Machine Learning\n",
    "### Gruppo STM: Luca Moresca, Valerio Santini, Nicholas Suozzi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23404c",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This project addresses the AML Challenge on **text → latent visual space** mapping, with the goal of producing textual embeddings that match the image embeddings generated by a predefined VAE.\n",
    "The architecture follows a VSE++ approach enriched with a multi-slot mechanism to achieve finer-grained alignment. The main model (TextToVis) projects the textual embedding into the visual space, while an auxiliary module (SlotAuxHead) generates multiple “slot” vectors, useful only during training to improve discrimination and internal diversity of representations.\n",
    "\n",
    "The training uses a combination of contrastive losses:\n",
    "- **Global triplet loss** (classic VSE++)\n",
    "- **Slot-based triplet loss** (max-over-slot)\n",
    "- **InfoNCE on slots**  \n",
    "- **ISDL** to prevent collapse between slots  \n",
    "- **Condensation loss** to maintain consistency between global and slot embeddings  \n",
    "\n",
    "This configuration allows for more robust and accurate alignment, reducing dependence on a single global vector and leveraging more granular signals.\n",
    "\n",
    "The pipeline includes:\n",
    "- a stable image level split, to ensure correct validation\n",
    "- saving checkpoints for each epoch and selecting the best one based on validation metrics\n",
    "- qualitative analysis through visual retrieval\n",
    "- an ensemble strategy of models trained with different seeds, used in inference to increase stability and performance\n",
    "\n",
    "The result is a system optimised to maximise metrics, combining classic metric learning techniques with modern mechanisms based on multi-slot representations and diversity regularisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce98e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from scipy import sparse\n",
    "\n",
    "from challenge.src.common import load_data, prepare_train_data, generate_submission\n",
    "from challenge.src.eval.metrics import mrr, recall_at_k, ndcg\n",
    "from challenge.src.eval.visualize import visualize_retrieval\n",
    "\n",
    "RUN_ID = 3\n",
    "\n",
    "MODEL_PATH = f\"models/vsepp_text2vis_best_seed{RUN_ID}.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 24\n",
    "BATCH_SIZE = 128\n",
    "LR_INIT = 2e-4\n",
    "LR_LATE = 2e-5\n",
    "LR_STEP_EPOCH = 16\n",
    "WEIGHT_DECAY = 0.0\n",
    "MARGIN = 0.20\n",
    "\n",
    "K_SLOTS_AUX = 4\n",
    "TAU_SLOT = 0.09\n",
    "LAMBDA_SLOT = 0.15\n",
    "\n",
    "USE_SLOT_TRIPLET = True\n",
    "LAMBDA_TRI_GLOBAL = 0.30\n",
    "\n",
    "LAMBDA_ISDL = 0.02\n",
    "ISDL_DELTA = 0.55\n",
    "\n",
    "LAMBDA_COND = 0.05\n",
    "\n",
    "SEED = 42 + RUN_ID\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3c31d",
   "metadata": {},
   "source": [
    "### Image-based split train/validation\n",
    "\n",
    "The dataset is split using an image-level split, so that all captions for the same image go into the same set.\n",
    "\n",
    "- The embeddings are loaded (X for text, y for image).\n",
    "- Each image is assigned to validation using a stable MD5 hash.\n",
    "- Captions inherit the split of their image.\n",
    "- Dedicated DataLoaders are built for training/validation.\n",
    "- For validation, the image gallery (val_img_embd) and val_label mapping, necessary for calculating the MRR, are also prepared.\n",
    "\n",
    "A reproducible split is then created, trying to avoid leakage between training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31069cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"data/train/train.npz\")\n",
    "X, y, label = prepare_train_data(train_data)\n",
    "\n",
    "img_names_all = train_data['images/names']\n",
    "img_emb_all = torch.from_numpy(train_data['images/embeddings']).float()\n",
    "\n",
    "val_ratio = 0.10\n",
    "def stable_hash(name: str) -> float:\n",
    "    h = hashlib.md5(name.encode('utf-8')).hexdigest()\n",
    "    return int(h[:8], 16) / 0xFFFFFFFF\n",
    "\n",
    "img_hash = np.array([stable_hash(nm) for nm in img_names_all])\n",
    "IMG_VAL_MASK = (img_hash < val_ratio)\n",
    "IMG_TRAIN_MASK = ~IMG_VAL_MASK\n",
    "\n",
    "cap_to_img = train_data['captions/label']\n",
    "if sparse.issparse(cap_to_img):\n",
    "    cap_gt_img_idx = cap_to_img.argmax(axis=1).A1\n",
    "else:\n",
    "    cap_gt_img_idx = np.argmax(cap_to_img, axis=1)\n",
    "\n",
    "CAP_TRAIN_MASK = IMG_TRAIN_MASK[cap_gt_img_idx]\n",
    "CAP_VAL_MASK = IMG_VAL_MASK[cap_gt_img_idx]\n",
    "\n",
    "assert not (IMG_TRAIN_MASK & IMG_VAL_MASK).any(), \"Overlap immagini train/val > 0\"\n",
    "\n",
    "X_train = X[CAP_TRAIN_MASK]\n",
    "y_train = y[CAP_TRAIN_MASK]\n",
    "X_val = X[CAP_VAL_MASK]\n",
    "y_val = y[CAP_VAL_MASK]\n",
    "\n",
    "print(f\"Train captions: {len(X_train):,} | Val captions: {len(X_val):,}\")\n",
    "print(f\"Train images: {int(IMG_TRAIN_MASK.sum()):,} | Val images: {int(IMG_VAL_MASK.sum()):,}\")\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=(DEVICE.type=='cuda'))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=(DEVICE.type=='cuda'))\n",
    "\n",
    "val_img_embd = F.normalize(img_emb_all[torch.from_numpy(IMG_VAL_MASK)], dim=-1).cpu()\n",
    "val_img_file = img_names_all[IMG_VAL_MASK]\n",
    "\n",
    "global_to_val = -np.ones(len(img_names_all), dtype=np.int64)\n",
    "global_to_val[np.where(IMG_VAL_MASK)[0]] = np.arange(IMG_VAL_MASK.sum(), dtype=np.int64)\n",
    "val_gt_global = cap_gt_img_idx[CAP_VAL_MASK]\n",
    "val_label = global_to_val[val_gt_global].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3438f4c",
   "metadata": {},
   "source": [
    "### Main model and auxiliary slot head\n",
    "\n",
    "**TextToVis**  \n",
    "This is the main adapter: a small MLP that projects the textual embedding into visual space (d_vis).  \n",
    "The output is normalised to use cosine similarity during training and retrieval.\n",
    "\n",
    "**SlotAuxHead**  \n",
    "Auxiliary head that generates K vectors in visual space from the same textual embedding.  \n",
    "These slots are only used during training (InfoNCE, triplet on slot, ISDL) to provide a richer and more fine-grained signal, but are not used in inference.\n",
    "\n",
    "Both networks are lightweight and completely separate:  \n",
    "- model produces the final vector to be used in submission  \n",
    "- slot_head produces the slots used as regularisation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca15f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToVis(nn.Module):\n",
    "    def __init__(self, d_text=1024, d_vis=1536, hidden=2048):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(d_text, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, d_vis, bias=True)\n",
    "        )\n",
    "    def forward(self, t):\n",
    "        z = self.proj(t)\n",
    "        return F.normalize(z, dim=-1)\n",
    "\n",
    "class SlotAuxHead(nn.Module):\n",
    "    \"\"\"Head ausiliaria: produce K vettori per InfoNCE\"\"\"\n",
    "    def __init__(self, d_text=1024, d_vis=1536, K=4, hidden=1024):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_text, hidden), nn.GELU(),\n",
    "            nn.Linear(hidden, K * d_vis, bias=True)\n",
    "        )\n",
    "        self.d_vis = d_vis\n",
    "    def forward(self, t):\n",
    "        B = t.size(0)\n",
    "        S = self.ff(t).view(B, self.K, self.d_vis)\n",
    "        return F.normalize(S, dim=-1)\n",
    "\n",
    "model = TextToVis(d_text=X.shape[1], d_vis=y.shape[1], hidden=2048).to(DEVICE)\n",
    "slot_head = SlotAuxHead(d_text=X.shape[1], d_vis=y.shape[1], K=K_SLOTS_AUX, hidden=1024).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84592e",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "\n",
    "This section lists all the losses used to guide the alignment between textual and visual embeddings. The standard triplet (triplet_hard_neg_cos) works on the global vector Z and follows the VSE++ setting, comparing each positive with the most similar negative in the batch. The triplet_hard_neg_slot_max variant extends the same logic to slots, using the maximum similarity between them to capture the best possible alignment: this approach is more sensitive to details and favours better Rank-1.\n",
    "\n",
    "To support this mechanism, *condensation loss* is introduced, which pushes Z closer to the slot that best matches its image, thus transferring the fine-grained accuracy of the slots to the final vector used in inference. loss_slot_ce, on the other hand, provides a direct contrastive signal for each slot, exploiting in-batch negatives.\n",
    "\n",
    "Finally, isdl_hinge keeps the slots diversified by penalising those that are too similar to each other, stabilising multi-slot learning and ensuring that the information captured is not redundant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73dbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_hard_neg_cos(Z, Y, margin):\n",
    "    \"\"\"Triplet VSE++ standard sul vettore globale Z\"\"\"\n",
    "    S = Z @ Y.t()\n",
    "    pos = S.diag()\n",
    "    B = S.size(0)\n",
    "    S = S.clone()\n",
    "    S[torch.arange(B), torch.arange(B)] = float('-inf')\n",
    "    neg = S.max(dim=1).values\n",
    "    return F.relu(margin + neg - pos).mean()\n",
    "\n",
    "def triplet_hard_neg_slot_max(S_T, Y, margin):\n",
    "    \"\"\"MPAS-lite: Triplet con hard negative usando la similarità max over slot\"\"\"\n",
    "    B, K, D = S_T.shape\n",
    "    sims = torch.einsum('bkd,nd->bkn', S_T, Y) \n",
    "    pos_all = sims[:, :, torch.arange(B)]\n",
    "    pos_max, _ = pos_all.max(dim=1)\n",
    "    sims = sims.clone()\n",
    "    idx = torch.arange(B, device=S_T.device)\n",
    "    sims[:, :, idx] = float('-inf')\n",
    "    neg_max, _ = sims.view(B, -1).max(dim=1)\n",
    "\n",
    "    return F.relu(margin + neg_max - pos_max).mean()\n",
    "\n",
    "def loss_condensation(Z, S_T, Y):\n",
    "    \"\"\"Condensation: forza Z ad assomigliare allo slot che meglio allinea Y\"\"\"\n",
    "    sims_ty = torch.einsum('bkd,bd->bk', S_T, Y)\n",
    "    best_idx = sims_ty.max(dim=1).indices\n",
    "    s_star = S_T[torch.arange(S_T.size(0), device=S_T.device), best_idx]\n",
    "    z_n = F.normalize(Z, dim=-1)\n",
    "    s_n = F.normalize(s_star, dim=-1)\n",
    "    cos_zs = torch.sum(z_n * s_n, dim=1)\n",
    "\n",
    "    return (1.0 - cos_zs).mean()\n",
    "\n",
    "def loss_slot_ce(S_T, Y, tau=0.07):\n",
    "    \"\"\"CE/InfoNCE per slot con negativi in batch\"\"\"\n",
    "    B, K, D = S_T.shape\n",
    "    Yt = Y.t()\n",
    "    losses = []\n",
    "    target = torch.arange(B, device=S_T.device)\n",
    "    for k in range(K):\n",
    "        logits = (S_T[:, k, :] @ Yt) / tau\n",
    "        losses.append(F.cross_entropy(logits, target))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def isdl_hinge(S_T, delta=0.55):\n",
    "    \"\"\"Penalizza solo sim intra slot > sigma\"\"\"\n",
    "    B, K, D = S_T.shape\n",
    "    if K <= 1:\n",
    "        return S_T.new_zeros(())\n",
    "    C = (S_T @ S_T.transpose(1, 2)).clamp(-1, 1)\n",
    "    mask = ~torch.eye(K, device=S_T.device, dtype=torch.bool)\n",
    "    C_off = C[:, mask].view(B, -1)\n",
    "    return F.relu(C_off - delta).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4260359",
   "metadata": {},
   "source": [
    "### Retrieval performance evaluation\n",
    "\n",
    "These functions are used to efficiently measure the quality of text-image alignment during training.\n",
    "\n",
    "evaluate_retrieval_global: calculates the similarities between all predicted vectors Z and the image gallery, processing them in blocks. For each query, it obtains the top-k most similar indices and from these derives MRR, NDCG and the various recall@k values. An average L2 distance from the ground truth is also estimated, which is useful as a rough indicator of the geometric consistency of the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_retrieval_global(Z: torch.Tensor,gallery: torch.Tensor,gt_indices: np.ndarray,topk: int = 100,chunk: int = 512):\n",
    "    Z = Z.to('cpu'); gallery = gallery.to('cpu')\n",
    "    Nq, Ng = Z.size(0), gallery.size(0)\n",
    "    topk = min(topk, Ng)\n",
    "\n",
    "    all_topk = []\n",
    "    for start in range(0, Nq, chunk):\n",
    "        end = min(start + chunk, Nq)\n",
    "        sims = Z[start:end] @ gallery.T\n",
    "        topk_idx = torch.topk(sims, k=topk, dim=1, largest=True, sorted=True).indices\n",
    "        all_topk.append(topk_idx.cpu().numpy())\n",
    "    pred_indices = np.vstack(all_topk).astype(np.int64)\n",
    "\n",
    "    l2_dist = (Z - gallery[torch.from_numpy(gt_indices)]).norm(dim=1).mean().item()\n",
    "    return {\n",
    "        'mrr': mrr(pred_indices, gt_indices),\n",
    "        'ndcg': ndcg(pred_indices, gt_indices),\n",
    "        'recall_at_1':  recall_at_k(pred_indices, gt_indices, 1),\n",
    "        'recall_at_3':  recall_at_k(pred_indices, gt_indices, 3),\n",
    "        'recall_at_5':  recall_at_k(pred_indices, gt_indices, 5),\n",
    "        'recall_at_10': recall_at_k(pred_indices, gt_indices, 10),\n",
    "        'recall_at_50': recall_at_k(pred_indices, gt_indices, 50),\n",
    "        'l2_dist': l2_dist,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48e9bd",
   "metadata": {},
   "source": [
    "### Model training \n",
    "\n",
    "The train_model function manages the entire training process by combining the main model and the slot head. Optimisation is performed using Adam, while the learning rate is modulated with CosineAnnealing to make the descent more stable in the last epochs.\n",
    "\n",
    "During each epoch, the model processes the batches, generating both the global vector Z and the slots S_T. The different components of the loss (global triplet, triplet on slots, InfoNCE on slots, ISDL and, if active, condensation loss) are combined to guide the fine-grained alignment between text and image. After backpropagation, the parameters of both modules are updated consistently.\n",
    "\n",
    "At the end of the epoch, the model is evaluated on the entire validation set: first, a positive average similarity is calculated, then normalised embeddings are generated for use in retrieval. The evaluate_retrieval_global function returns all metrics. Training continues until the predefined number of epochs is completed, finally returning the model with the best performance on the validation gallery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module,train_loader: DataLoader,val_loader: DataLoader,device: torch.device,epochs: int) -> nn.Module:\n",
    "    opt = torch.optim.Adam(list(model.parameters()) + list(slot_head.parameters()), lr=LR_INIT, weight_decay=WEIGHT_DECAY)\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS * steps_per_epoch, eta_min=LR_LATE)\n",
    "    best_mrr = -1.0\n",
    "    ckpt_dir = Path(MODEL_PATH).parent / f\"checkpoints_vsepp_seed{RUN_ID}\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for Xb, Yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
    "            Xb = Xb.to(device, non_blocking=True)\n",
    "            Yb = Yb.to(device, non_blocking=True)\n",
    "            Zb = model(Xb)\n",
    "            Sb = slot_head(Xb)\n",
    "\n",
    "            if USE_SLOT_TRIPLET:\n",
    "                loss_tri_slot = triplet_hard_neg_slot_max(Sb, Yb, margin=MARGIN)\n",
    "                loss_tri_global = triplet_hard_neg_cos(Zb, Yb, margin=MARGIN)\n",
    "                loss_tri = loss_tri_slot + LAMBDA_TRI_GLOBAL * loss_tri_global\n",
    "            else:\n",
    "                loss_tri = triplet_hard_neg_cos(Zb, Yb, margin=MARGIN)\n",
    "\n",
    "            loss_slot = loss_slot_ce(Sb, Yb, tau=TAU_SLOT)\n",
    "\n",
    "            if LAMBDA_ISDL > 0.0:\n",
    "                loss_isd = isdl_hinge(Sb, delta=ISDL_DELTA)\n",
    "            else:\n",
    "                loss_isd = Sb.new_zeros(())\n",
    "\n",
    "            if LAMBDA_COND > 0.0 and USE_SLOT_TRIPLET:\n",
    "                loss_cond = loss_condensation(Zb, Sb, Yb)\n",
    "            else:\n",
    "                loss_cond = Zb.new_zeros(())\n",
    "\n",
    "            loss = (loss_tri + LAMBDA_SLOT * loss_slot + LAMBDA_ISDL * loss_isd + LAMBDA_COND * loss_cond)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(list(model.parameters()) + list(slot_head.parameters()), 1.0)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "        val_cos_sum, val_batches = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for Xb, Yb in DataLoader(val_loader.dataset, batch_size=BATCH_SIZE, shuffle=False):\n",
    "                Xb = Xb.to(device, non_blocking=True)\n",
    "                Yb = F.normalize(Yb.to(device, non_blocking=True), dim=-1)\n",
    "                Zb = model(Xb)\n",
    "                cos_b = torch.einsum('bd,bd->b', Zb, Yb).mean()\n",
    "                val_cos_sum += float(cos_b.item())\n",
    "                val_batches += 1\n",
    "\n",
    "        preds_val = []\n",
    "        with torch.no_grad():\n",
    "            for Xb, _ in DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False):\n",
    "                Xb = Xb.to(device, non_blocking=True)\n",
    "                preds_val.append(model(Xb).cpu())\n",
    "        Z_val = F.normalize(torch.cat(preds_val, dim=0), dim=-1).cpu()\n",
    "\n",
    "        res_val = evaluate_retrieval_global(Z_val, val_img_embd, val_label, topk=100, chunk=512)\n",
    "        curr_mrr = res_val['mrr']\n",
    "\n",
    "        print(f\"losses: tri={float(loss_tri.item()):.4f} | \"\n",
    "              f\"slot={float(loss_slot.item()):.4f} | \"\n",
    "              f\"isd={float(loss_isd.item()):.4f} | \"\n",
    "              f\"cond={float(loss_cond.item()):.4f}\")\n",
    "\n",
    "        ckpt_path = ckpt_dir / f\"epoch_{epoch:03d}.pth\"\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Saved {ckpt_path.name}\")\n",
    "\n",
    "        if curr_mrr > best_mrr:\n",
    "            best_mrr = curr_mrr\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New best MRR ({best_mrr:.5f}),  saved {Path(MODEL_PATH).name}\")\n",
    "\n",
    "    print(f\"\\nTraining finito. Best MRR val: {best_mrr:.5f}, checkpoint: {MODEL_PATH}\")\n",
    "    return model\n",
    "\n",
    "print(f\"RUN_ID = {RUN_ID}, SEED = {SEED}\")\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7047a91",
   "metadata": {},
   "source": [
    "### Preparation of the validation gallery and evaluation functions\n",
    "\n",
    "After training, the model is switched to evaluation mode and the same image split used previously is reconstructed. This ensures that the validation gallery remains identical to the one seen during training and that no leakage occurs between training and validation.\n",
    "\n",
    "All captions associated with the validation images are then selected, the relevant DataLoader is reconstructed, and the embeddings of the gallery images are normalised. For each caption, the exact index of the correct image within the gallery is also calculated: this mapping (val_label) is essential for measuring MRR.\n",
    "\n",
    "The evaluate_retrieval_global function performs the actual retrieval: it calculates the similarities between the predicted vectors and the gallery, extracts the top-k most similar ones and, from these, computes MRR, NDCG and recall@k. The calculation is done in blocks to avoid memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4796ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(int.from_bytes(os.urandom(4), \"little\"))\n",
    "model.eval()\n",
    "\n",
    "img_names_all = train_data['images/names']\n",
    "img_emb_all = torch.from_numpy(train_data['images/embeddings']).float()\n",
    "\n",
    "val_ratio = 0.10\n",
    "\n",
    "img_hash = np.array([stable_hash(nm) for nm in img_names_all])\n",
    "IMG_VAL_MASK = (img_hash < val_ratio)\n",
    "IMG_TRAIN_MASK = ~IMG_VAL_MASK\n",
    "assert not (IMG_VAL_MASK & IMG_TRAIN_MASK).any(), \"Overlap immagini train/val > 0\"\n",
    "\n",
    "cap_to_img = train_data['captions/label']\n",
    "if sparse.issparse(cap_to_img):\n",
    "    cap_gt_img_idx = cap_to_img.argmax(axis=1).A1\n",
    "else:\n",
    "    cap_gt_img_idx = np.argmax(cap_to_img, axis=1)\n",
    "\n",
    "CAP_TRAIN_MASK = IMG_TRAIN_MASK[cap_gt_img_idx]\n",
    "CAP_VAL_MASK = IMG_VAL_MASK[cap_gt_img_idx]\n",
    "\n",
    "X_val = X[CAP_VAL_MASK]\n",
    "y_val = y[CAP_VAL_MASK]\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=(DEVICE.type=='cuda'))\n",
    "\n",
    "print(f\"Val captions: {len(X_val):,}, Val images: {int(IMG_VAL_MASK.sum()):,}\")\n",
    "\n",
    "val_img_embd = F.normalize(img_emb_all[torch.from_numpy(IMG_VAL_MASK)], dim=-1).cpu()  # (N_img_val, D)\n",
    "val_img_file = img_names_all[IMG_VAL_MASK]\n",
    "\n",
    "global_to_val = -np.ones(len(img_names_all), dtype=np.int64)\n",
    "global_to_val[np.where(IMG_VAL_MASK)[0]] = np.arange(IMG_VAL_MASK.sum(), dtype=np.int64)\n",
    "val_gt_global = cap_gt_img_idx[CAP_VAL_MASK]\n",
    "val_label = global_to_val[val_gt_global]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "preds_val = []\n",
    "with torch.no_grad():\n",
    "    for Xb, _ in DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False):\n",
    "        Xb = Xb.to(DEVICE, non_blocking=True)\n",
    "        Eb = model(Xb)\n",
    "        preds_val.append(Eb.cpu())\n",
    "Z_val_best = F.normalize(torch.cat(preds_val, dim=0), dim=-1).cpu()\n",
    "\n",
    "res_val = evaluate_retrieval_global(Z_val_best, val_img_embd, val_label, topk=100, chunk=512)\n",
    "print(\"\\nVal (image-level split, global gallery) — BEST CKPT (seed corrente)\")\n",
    "for k, v in res_val.items():\n",
    "    print(f\"{k:15s}: {v:.4f}\")\n",
    "\n",
    "for _ in range(3):\n",
    "    i = np.random.randint(0, len(X_val))\n",
    "    with torch.no_grad():\n",
    "        zb = model(X_val[i:i+1].to(DEVICE)).cpu()\n",
    "    caption_text = train_data['captions/text'][CAP_VAL_MASK][i]\n",
    "    gt_idx = int(val_label[i])\n",
    "    visualize_retrieval(zb, gt_idx, val_img_file, caption_text, val_img_embd, k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5580d2",
   "metadata": {},
   "source": [
    "### Ensemble of the best models and submission generation\n",
    "\n",
    "To improve the quality of the final embeddings, an ensemble of models trained with different seeds can be used.  \n",
    "Each checkpoint is loaded into a separate instance of TextToVis, kept in eval mode, so as to combine independent but architecturally identical predictions. In this notebook we use a single seed for simplicity, but the same code supports multiple checkpoints (e.g., 3–6 seeds).\n",
    "\n",
    "During inference on the test set, each batch of text embeddings is passed to all models in the ensemble; their outputs are summed and then averaged. Final normalisation ensures that the resulting embeddings remain compatible with the cosine similarity used in evaluation.\n",
    "\n",
    "The embeddings produced are concatenated and verified in their final form, then saved in the format required by the challenge using generate_submission.  \n",
    "The result is a more stable and robust submission than when using a single model, especially when multiple seeds are active.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_ENSEMBLE = [\n",
    "    # \"models/vsepp_text2vis_best_seed0.pth\",\n",
    "    # \"models/vsepp_text2vis_best_seed1.pth\",\n",
    "    # \"models/vsepp_text2vis_best_seed2.pth\",\n",
    "    \"models/vsepp_text2vis_best_seed3.pth\",\n",
    "    # \"models/vsepp_text2vis_best_seed4.pth\",\n",
    "    # \"models/vsepp_text2vis_best_seed5.pth\",\n",
    "]\n",
    "\n",
    "def build_model_for_inference():\n",
    "    m = TextToVis(d_text=X.shape[1], d_vis=y.shape[1], hidden=2048).to(DEVICE)\n",
    "    return m\n",
    "\n",
    "models_ens = []\n",
    "for ckpt_path in CHECKPOINTS_ENSEMBLE:\n",
    "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    m = build_model_for_inference()\n",
    "    m.load_state_dict(state)\n",
    "    m.eval()\n",
    "    models_ens.append(m)\n",
    "    print(f\"Caricato per ensemble: {ckpt_path}\")\n",
    "\n",
    "test = load_data(\"data/test/test.clean.npz\")\n",
    "test_ids = test['captions/ids']\n",
    "test_emb = torch.from_numpy(test['captions/embeddings']).float()\n",
    "\n",
    "pred_chunks = []\n",
    "with torch.inference_mode():\n",
    "    for Xb in DataLoader(test_emb, batch_size=BATCH_SIZE, shuffle=False, pin_memory=(DEVICE.type=='cuda')):\n",
    "        Xb = Xb.to(DEVICE, non_blocking=True)\n",
    "        Z_agg = None\n",
    "        for m in models_ens:\n",
    "            Zb = m(Xb)\n",
    "            Z_agg = Zb if Z_agg is None else (Z_agg + Zb)\n",
    "\n",
    "        Z_agg = Z_agg / len(models_ens)\n",
    "        Z_agg = F.normalize(Z_agg, dim=-1)\n",
    "        pred_chunks.append(Z_agg.cpu().to(torch.float32))\n",
    "\n",
    "pred_test = torch.cat(pred_chunks, dim=0)\n",
    "\n",
    "assert pred_test.shape[0] == len(test_ids)\n",
    "assert pred_test.shape[1] == y.shape[1], f\"{pred_test.shape[1]} != {y.shape[1]} (D_img)\"\n",
    "\n",
    "generate_submission(test_ids, pred_test, \"submission_ensemble.csv\")\n",
    "print(\"Submission pronta: submission_ensemble.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
