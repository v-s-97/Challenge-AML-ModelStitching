{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mamiglia/challenge/blob/master/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SYldJ58Zc2gX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYldJ58Zc2gX",
        "outputId": "ac87a904-31f1-4aa9-e8b4-fe1159c48c26"
      },
      "outputs": [],
      "source": [
        "# !mkdir data\n",
        "# !gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
        "# !gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
        "# !unzip -o test.zip -d data\n",
        "# !unzip -o train.zip -d data\n",
        "\n",
        "# !git clone https://github.com/Mamiglia/challenge.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9a8587",
      "metadata": {
        "id": "9c9a8587"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import torch.nn.functional as F \n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from src.common import load_data, prepare_train_data, generate_submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5e8e20",
      "metadata": {
        "id": "9f5e8e20"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_PATH = \"models/mlp_baseline.pth\"\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "LR = 1e-4\n",
        "\n",
        "DEVICE = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c77dba3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class KernelAdapter(nn.Module):\n",
        "    \"\"\"Base kernel adapter with Random Fourier Features\"\"\"\n",
        "    def __init__(self, d_in, d_out, n_feats=256, sigma=0.8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(d_in, n_feats) / sigma)\n",
        "        self.b = nn.Parameter(torch.rand(n_feats) * 2 * math.pi)\n",
        "        self.fc = nn.Linear(n_feats, d_out)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_proj = torch.cos(x @ self.W + self.b)\n",
        "        x_proj = self.drop(x_proj)\n",
        "        out = self.fc(x_proj)\n",
        "        return F.normalize(out, dim=-1)\n",
        "\n",
        "\n",
        "class KernelResidualAdapter(nn.Module):\n",
        "    \"\"\"Kernel adapter + residual MLP + local geometric correction\"\"\"\n",
        "    def __init__(self, d_in=1024, d_out=1536, n_feats=512, sigma=0.7):\n",
        "        super().__init__()\n",
        "        self.kernel = KernelAdapter(d_in, d_out, n_feats, sigma)\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Linear(d_in, d_out),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_out, d_out)\n",
        "        )\n",
        "        # Local geometric correction (small feature-wise modulation)\n",
        "        self.local = nn.Sequential(\n",
        "            nn.Linear(d_in, d_out // 8),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_out // 8, d_out),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Amplitude scaling gate\n",
        "        self.scale = nn.Sequential(\n",
        "            nn.Linear(d_in, d_out),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # ðŸ”¹ Affine alignment layer (Procrustes-like)\n",
        "        self.align = nn.Linear(d_out, d_out, bias=False)\n",
        "        nn.init.eye_(self.align.weight)  # inizializza come identitÃ \n",
        "\n",
        "    def forward(self, x):\n",
        "        base = self.kernel(x)\n",
        "        res = self.residual(x)\n",
        "        local = self.local(x) * 0.3  # local correction term (scaled)\n",
        "        scale = self.scale(x)\n",
        "\n",
        "        # composizione finale\n",
        "        out = (base + 0.5 * res + local) * scale\n",
        "        out = self.align(out)  # affine rotation layer\n",
        "\n",
        "        # normalizzazione su sfera unitaria\n",
        "        # return F.normalize(out, dim=-1)\n",
        "        return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8952b1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_loss(z_pred, z_true, temperature=0.15, alpha=0.2, beta=0.6, gamma=1.5, hard_neg_weight=0.1):\n",
        "    # === Allineamento direzionale ===\n",
        "    mse = F.mse_loss(z_pred, z_true)\n",
        "    cos = 1 - F.cosine_similarity(z_pred, z_true, dim=-1).mean()\n",
        "\n",
        "    # === InfoNCE standard ===\n",
        "    z_pred = F.normalize(z_pred, dim=-1)\n",
        "    z_true = F.normalize(z_true, dim=-1)\n",
        "    logits = (z_pred @ z_true.T) / temperature\n",
        "    labels = torch.arange(logits.size(0), device=z_pred.device)\n",
        "    contrast = F.cross_entropy(logits, labels)\n",
        "\n",
        "    # === Penalizzazione hard negatives (solo nei logits â€œviciniâ€) ===\n",
        "    with torch.no_grad():\n",
        "        sim_ranks = logits.topk(k=5, dim=1).indices\n",
        "    hard_mask = torch.zeros_like(logits)\n",
        "    hard_mask.scatter_(1, sim_ranks, 1.0)\n",
        "    hard_neg = ((hard_mask * torch.exp(logits)).sum(1)).mean()\n",
        "\n",
        "    return alpha*mse + beta*cos + gamma*contrast + hard_neg_weight * hard_neg\n",
        "\n",
        "def align_uniform_loss(z1, z2, alpha=2, lam=0.1):\n",
        "    # alignment: rendi vicini i positivi\n",
        "    align = (z1 - z2).pow(2).sum(1).pow(alpha / 2).mean()\n",
        "\n",
        "    # uniformity: calcolo manuale delle distanze\n",
        "    z1 = F.normalize(z1, dim=-1)\n",
        "    sim_matrix = z1 @ z1.T\n",
        "    sq_dist = 2 - 2 * sim_matrix  # ||x_i - x_j||^2 = 2(1 - cos)\n",
        "    uniform = torch.log(torch.exp(-lam * sq_dist).mean())\n",
        "    \n",
        "    return align + uniform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac46c8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def metric_reg(z_pred, z_text, k=30):\n",
        "    \"\"\"Forza coerenza locale tra spazi: se due caption sono simili in A, lo siano anche in B.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        sims_text = (z_text @ z_text.T)\n",
        "        idx = sims_text.topk(k, dim=1).indices\n",
        "        mask = torch.zeros_like(sims_text)\n",
        "        mask.scatter_(1, idx, 1.0)\n",
        "    sims_pred = (z_pred @ z_pred.T)\n",
        "    return ((mask * (sims_pred - sims_text))**2).mean()\n",
        "\n",
        "def relational_alignment_loss(z_pred, S_target, max_points=512):\n",
        "    \"\"\"\n",
        "    Encourage predicted space to preserve the target (VAE) relational geometry.\n",
        "    S_target: precomputed [M, M] similarity matrix (subset used automatically)\n",
        "    \"\"\"\n",
        "    N = z_pred.shape[0]\n",
        "    M = S_target.shape[0]\n",
        "\n",
        "    # Se batch piÃ¹ grande della matrice target, usa solo primi punti\n",
        "    if N > M:\n",
        "        z_pred = z_pred[:M]\n",
        "        N = M\n",
        "\n",
        "    # Estrai un sottoinsieme coerente da S_target (prima N righe e colonne)\n",
        "    S_sub = S_target[:N, :N].to(z_pred.device)\n",
        "\n",
        "    # Calcola struttura predetta\n",
        "    z_pred = F.normalize(z_pred, dim=-1)\n",
        "    S_pred = z_pred @ z_pred.T  # [N, N]\n",
        "\n",
        "    # MSE fra le due strutture\n",
        "    loss = F.mse_loss(S_pred, S_sub)\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f048007",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_geodesic_similarity(Y, scale=0.5, max_points=2000):\n",
        "    \"\"\"\n",
        "    Compute geodesic similarity with automatic sigma adaptation.\n",
        "\n",
        "    scale: multiplicative factor to adjust sigma relative to mean distance.\n",
        "           smaller -> more contrast, larger -> smoother.\n",
        "    \"\"\"\n",
        "    if len(Y) > max_points:\n",
        "        idx = torch.randperm(len(Y))[:max_points]\n",
        "        Y = Y[idx]\n",
        "\n",
        "    # Pairwise distances\n",
        "    dist2 = torch.cdist(Y, Y, p=2).pow(2)\n",
        "    mean_d2 = dist2.mean().item()\n",
        "    sigma = (mean_d2 ** 0.5) * scale   # e.g. 0.5 Ã— mean distance\n",
        "    print(f\"[auto-sigma] mean distanceÂ²={mean_d2:.4f}, sigma={sigma:.4f}\")\n",
        "\n",
        "    S = torch.exp(-dist2 / (2 * sigma**2))\n",
        "    S = S / S.max()\n",
        "    return S\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f083bfda",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_text_similarity(X, max_points=2000):\n",
        "    if len(X) > max_points:\n",
        "        idx = torch.randperm(len(X))[:max_points]\n",
        "        X = X[idx]\n",
        "    S = (F.cosine_similarity(X.unsqueeze(1), X.unsqueeze(0), dim=-1) + 1) / 2\n",
        "    return S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6883251f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def parallel_transport_loss(z_pred, z_text):\n",
        "    if z_text.size(0) < 2:\n",
        "        return torch.tensor(0.0, device=z_pred.device)\n",
        "    dz = z_text[1:] - z_text[:-1]\n",
        "    dp = z_pred[1:] - z_pred[:-1]\n",
        "    dz = F.normalize(dz, dim=-1)\n",
        "    dp = F.normalize(dp, dim=-1)\n",
        "    # riduci alla dimensione minima comune\n",
        "    d = min(dz.shape[-1], dp.shape[-1])\n",
        "    dz, dp = dz[..., :d], dp[..., :d]\n",
        "    return 1 - F.cosine_similarity(dp, dz, dim=-1).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30cbff24",
      "metadata": {
        "id": "30cbff24"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, device, epochs, lr, S_vae):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=5e-4,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=len(train_loader)\n",
        ")\n",
        "\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "\n",
        "            # === Loss composita ===\n",
        "            loss = (\n",
        "                hybrid_loss(outputs, y_batch)\n",
        "                + 0.2 * metric_reg(outputs, y_batch)\n",
        "                + 0.1 * align_uniform_loss(outputs, y_batch)\n",
        "                + 0.3 * relational_alignment_loss(outputs, S_vae)\n",
        "                + 0.05 * parallel_transport_loss(outputs, X_batch)\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        val_loss, cos_sim, mse, contrastive = 0, 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(X_batch)\n",
        "                loss = (\n",
        "                    hybrid_loss(outputs, y_batch)\n",
        "                    + 0.2 * metric_reg(outputs, y_batch)\n",
        "                    + 0.1 * align_uniform_loss(outputs, y_batch)\n",
        "                    + 0.1 * relational_alignment_loss(outputs, S_vae)\n",
        "                    + 0.05 * parallel_transport_loss(outputs, X_batch)\n",
        "                )\n",
        "                val_loss += loss.item()\n",
        "                cos_sim += F.cosine_similarity(outputs, y_batch, dim=-1).mean().item()\n",
        "                mse += F.mse_loss(outputs, y_batch).item()\n",
        "                sim = (F.normalize(outputs, dim=-1) @ F.normalize(y_batch, dim=-1).T)\n",
        "                pos = torch.arange(sim.size(0), device=device)\n",
        "                contrastive += F.cross_entropy(sim / 0.07, pos).item()\n",
        "\n",
        "        n_batches = len(val_loader)\n",
        "        val_loss /= n_batches\n",
        "        cos_sim /= n_batches\n",
        "        mse /= n_batches\n",
        "        contrastive /= n_batches\n",
        "\n",
        "        print(f\"Cosine={cos_sim:.4f}, MSE={mse:.4f}, Contrastive={contrastive:.4f}\")\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}: \"\n",
        "            f\"TrainLoss={train_loss:.4f}, ValLoss={val_loss:.4f} | \"\n",
        "            f\"MSE={mse:.4f}, Cosine={cos_sim:.4f}, Contrastive={contrastive:.4f}, \"\n",
        "            f\"MetricReg={metric_reg(outputs, y_batch):.4f}, \"\n",
        "            f\"AlignUniform={align_uniform_loss(outputs, y_batch):.4f}, \"\n",
        "            f\"Relational={relational_alignment_loss(outputs, S_vae):.4f}, \"\n",
        "            f\"PT={parallel_transport_loss(outputs, X_batch):.4f}\"\n",
        "        )\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            print(f\"  âœ“ Saved best model (val_loss={val_loss:.6f})\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f486748",
      "metadata": {},
      "outputs": [],
      "source": [
        "def whiten(X, eps=1e-6):\n",
        "    X = X - X.mean(0, keepdim=True)\n",
        "    cov = (X.T @ X) / (X.shape[0] - 1)\n",
        "    eigvals, eigvecs = torch.linalg.eigh(cov)\n",
        "    \n",
        "    # Evita divisione per 0\n",
        "    eigvals = torch.clamp(eigvals, min=eps)\n",
        "    \n",
        "    W = eigvecs @ torch.diag(1.0 / torch.sqrt(eigvals)) @ eigvecs.T\n",
        "    return X @ W\n",
        "\n",
        "# SVD VERSION \n",
        "# def whiten(X, eps=1e-6):\n",
        "#     \"\"\"\n",
        "#     Whiten data matrix X using SVD decomposition (numerically stable).\n",
        "\n",
        "#     Args:\n",
        "#         X (torch.Tensor): input data of shape [N, D]\n",
        "#         eps (float): small constant for numerical stability\n",
        "#     Returns:\n",
        "#         torch.Tensor: whitened data of shape [N, D]\n",
        "#     \"\"\"\n",
        "#     # 1ï¸âƒ£ Center the data (remove mean)\n",
        "#     X = X - X.mean(0, keepdim=True)\n",
        "    \n",
        "#     # 2ï¸âƒ£ Compute SVD (does not require explicit covariance)\n",
        "#     # X = U * S * Vh, where columns of Vh are principal directions\n",
        "#     U, S, Vh = torch.linalg.svd(X, full_matrices=False)\n",
        "    \n",
        "#     # 3ï¸âƒ£ Scale by inverse sqrt of singular values\n",
        "#     # (equivalent to dividing by std in each PCA direction)\n",
        "#     X_white = (U @ torch.diag(1.0 / torch.sqrt(S + eps))) @ Vh\n",
        "    \n",
        "#     # 4ï¸âƒ£ Optional: normalize each vector to unit norm\n",
        "#     X_white = F.normalize(X_white, dim=-1)\n",
        "    \n",
        "#     return X_white"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75dc85c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75dc85c5",
        "outputId": "6a8c8b95-388e-45fe-cf09-ef67e502b392"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "train_data = load_data(\"data/train/train.npz\")\n",
        "X, y, label = prepare_train_data(train_data)\n",
        "\n",
        "S_vae = compute_geodesic_similarity(y, scale=0.5)\n",
        "print(\"Computed S_vae with shape:\", S_vae.shape)\n",
        "plt.imshow(S_vae.cpu().numpy(), cmap='magma')\n",
        "plt.title(\"Approx. Geodesic Similarity (VAE latent space)\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# X = whiten(X)\n",
        "# y = whiten(y)\n",
        "X = F.normalize(X, dim=-1)\n",
        "y = F.normalize(y, dim=-1)\n",
        "\n",
        "\n",
        "# === Approximate Similarity in the Text Embedding Space ===\n",
        "S_text = compute_text_similarity(X, 512)\n",
        "print(\"Computed S_text with shape:\", S_text.shape)\n",
        "plt.imshow(S_text.cpu().numpy(), cmap='magma')\n",
        "plt.title(\"Text-space Similarity Matrix\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "DATASET_SIZE = len(X)\n",
        "# Split train/val\n",
        "# This is done only to measure generalization capabilities, you don't have to\n",
        "# use a validation set (though we encourage this)\n",
        "n_train = int(0.9 * len(X))\n",
        "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
        "TRAIN_SPLIT[:n_train] = 1\n",
        "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
        "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "y_train.shape[-1], X_val.shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963c0644",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "963c0644",
        "outputId": "2d9d58ff-e0af-4ab1-be34-71052cf00690"
      },
      "outputs": [],
      "source": [
        "model = KernelResidualAdapter().to(DEVICE)\n",
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train\n",
        "print(\"\\n3. Training...\")\n",
        "model = train_model(model, train_loader, val_loader, DEVICE, EPOCHS, LR, S_vae)\n",
        "\n",
        "# Load best model for evaluation\n",
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a533e6f",
      "metadata": {
        "id": "0a533e6f"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3319399",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def visualize_latent_space(model, X_val, y_val, device, method=\"pca\", n_samples=2000):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val = X_val[:n_samples].to(device)\n",
        "        y_val = y_val[:n_samples].to(device)\n",
        "        z_pred = model(X_val).cpu()\n",
        "        z_true = y_val.cpu()\n",
        "\n",
        "    if method == \"tsne\":\n",
        "        reducer = TSNE(n_components=2, perplexity=30, n_iter=1000, init=\"pca\")\n",
        "    else:\n",
        "        reducer = PCA(n_components=2)\n",
        "\n",
        "    z_pred_2d = reducer.fit_transform(z_pred)\n",
        "    z_true_2d = reducer.fit_transform(z_true)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(z_true_2d[:, 0], z_true_2d[:, 1], alpha=0.5, s=12, label=\"True (Target Space)\")\n",
        "    plt.scatter(z_pred_2d[:, 0], z_pred_2d[:, 1], alpha=0.5, s=12, label=\"Predicted (Mapped Space)\")\n",
        "    plt.title(f\"Latent Space Visualization ({method.upper()})\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_tsne(model, X_val, y_val, device, n_samples=2000, perplexity=30, seed=42):\n",
        "    \"\"\"\n",
        "    Visualizza le embedding predette e target nello spazio 2D con t-SNE.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val = X_val[:n_samples].to(device)\n",
        "        y_val = y_val[:n_samples].to(device)\n",
        "        z_pred = F.normalize(model(X_val), dim=-1).cpu().numpy()\n",
        "        z_true = F.normalize(y_val, dim=-1).cpu().numpy()\n",
        "\n",
        "    # Concatena per ottenere embedding congiunte\n",
        "    Z = np.concatenate([z_pred, z_true], axis=0)\n",
        "    labels = np.array([0] * len(z_pred) + [1] * len(z_true))  # 0=pred, 1=true\n",
        "\n",
        "    # Riduzione con t-SNE\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        perplexity=perplexity,\n",
        "        n_iter=1000,\n",
        "        init=\"pca\",\n",
        "        learning_rate=\"auto\",\n",
        "        random_state=seed,\n",
        "        verbose=1\n",
        "    )\n",
        "    Z_2d = tsne.fit_transform(Z)\n",
        "\n",
        "    # Split per colore\n",
        "    Z_pred_2d = Z_2d[labels == 0]\n",
        "    Z_true_2d = Z_2d[labels == 1]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(Z_true_2d[:, 0], Z_true_2d[:, 1],\n",
        "                alpha=0.45, s=18, c=\"#1f77b4\", label=\"True (target space)\")\n",
        "    plt.scatter(Z_pred_2d[:, 0], Z_pred_2d[:, 1],\n",
        "                alpha=0.45, s=18, c=\"#ff7f0e\", label=\"Predicted (mapped space)\")\n",
        "    plt.title(f\"t-SNE of Predicted vs True Embeddings ({n_samples} samples)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6322b10",
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def visualize_latent_space_3d(model, X_val, y_val, device, n_samples=2000, method=\"pca\"):\n",
        "    \"\"\"\n",
        "    Visualizza gli embedding predetti e target in 3D (interattivo) con Plotly.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val = X_val[:n_samples].to(device)\n",
        "        y_val = y_val[:n_samples].to(device)\n",
        "        z_pred = F.normalize(model(X_val), dim=-1).cpu().numpy()\n",
        "        z_true = F.normalize(y_val, dim=-1).cpu().numpy()\n",
        "\n",
        "    # Riduzione dimensionale (3D)\n",
        "    if method == \"tsne\":\n",
        "        from sklearn.manifold import TSNE\n",
        "        reducer = TSNE(n_components=3, perplexity=30, n_iter=1000, init=\"pca\", learning_rate=\"auto\")\n",
        "        Z_pred = reducer.fit_transform(z_pred)\n",
        "        Z_true = reducer.fit_transform(z_true)\n",
        "    else:\n",
        "        reducer = PCA(n_components=3)\n",
        "        Z_pred = reducer.fit_transform(z_pred)\n",
        "        Z_true = reducer.fit_transform(z_true)\n",
        "\n",
        "    # Creazione grafico interattivo\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Target space\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=Z_true[:,0], y=Z_true[:,1], z=Z_true[:,2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=3, color='royalblue', opacity=0.6),\n",
        "        name='True (Target Space)'\n",
        "    ))\n",
        "\n",
        "    # Predicted space\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=Z_pred[:,0], y=Z_pred[:,1], z=Z_pred[:,2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=3, color='orange', opacity=0.6),\n",
        "        name='Predicted (Mapped Space)'\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f\"3D Latent Space Visualization ({method.upper()})\",\n",
        "        scene=dict(\n",
        "            xaxis_title='Component 1',\n",
        "            yaxis_title='Component 2',\n",
        "            zaxis_title='Component 3'\n",
        "        ),\n",
        "        width=850,\n",
        "        height=700,\n",
        "        legend=dict(x=0.02, y=0.98)\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d060a9e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d060a9e8",
        "outputId": "3d91f177-0162-4da0-963a-93ba152a2b90"
      },
      "outputs": [],
      "source": [
        "from challenge.src.eval import visualize_retrieval\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "visualize_latent_space(model, X_val, y_val, DEVICE, method=\"pca\")\n",
        "visualize_tsne(model, X_val, y_val, DEVICE, n_samples=2000)\n",
        "\n",
        "\n",
        "visualize_latent_space_3d(model, X_val, y_val, DEVICE, n_samples=2000, method=\"pca\")\n",
        "visualize_latent_space_3d(model, X_val, y_val, DEVICE, n_samples=1000, method=\"tsne\")\n",
        "\n",
        "val_caption_text = train_data['captions/text'][~TRAIN_SPLIT]\n",
        "val_text_embd = X_val\n",
        "img_VAL_SPLIT = label[~TRAIN_SPLIT].sum(dim=0) > 0\n",
        "val_img_file = train_data['images/names'][img_VAL_SPLIT]\n",
        "val_img_embd = torch.from_numpy(train_data['images/embeddings'][img_VAL_SPLIT])\n",
        "val_label = np.nonzero(train_data['captions/label'][~TRAIN_SPLIT][:,img_VAL_SPLIT])[1]\n",
        "\n",
        "# Sample and visualize\n",
        "for i in range(5):\n",
        "    idx = np.random.randint(0, 100)\n",
        "    caption_embd = val_text_embd[idx]\n",
        "    caption_text = val_caption_text[idx]\n",
        "    gt_index = val_label[idx]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_embds = model(caption_embd.to(DEVICE)).cpu()\n",
        "\n",
        "        visualize_retrieval(\n",
        "            pred_embds,\n",
        "            gt_index,\n",
        "            val_img_file,\n",
        "            caption_text, val_img_embd, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09d07798",
      "metadata": {
        "id": "09d07798"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156b49a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "156b49a0",
        "outputId": "dc5070d3-f3a7-427b-e98c-76cbb957a4dd"
      },
      "outputs": [],
      "source": [
        "test_data = load_data(\"data/test/test.clean.npz\")\n",
        "\n",
        "test_embds = test_data['captions/embeddings']\n",
        "test_embds = torch.from_numpy(test_embds).float()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_embds = model(test_embds.to(DEVICE)).cpu()\n",
        "\n",
        "submission = generate_submission(test_data['captions/ids'], pred_embds, 'submission.csv')\n",
        "print(f\"Model saved to: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mVsCiM2MmLKE",
      "metadata": {
        "id": "mVsCiM2MmLKE"
      },
      "outputs": [],
      "source": [
        "# --- Evaluate retrieval performance ---\n",
        "from challenge.src.eval import evaluate_retrieval  # adattalo al tuo path\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_pred_val = model(X_val.to(DEVICE)).cpu()\n",
        "\n",
        "# ground truth embeddings (target images)\n",
        "z_img_val = y_val.cpu()\n",
        "gt_indices = np.arange(len(z_img_val))  # ogni caption ha la sua immagine target\n",
        "\n",
        "results = evaluate_retrieval(\n",
        "    translated_embd=z_pred_val,\n",
        "    image_embd=z_img_val,\n",
        "    gt_indices=gt_indices,\n",
        "    max_indices=50,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "print(\"\\n=== Retrieval evaluation (validation set) ===\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k:>12}: {v:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
